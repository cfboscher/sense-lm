{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a13939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/media/username/DATA_LINUX1/Datasets/Odeuropa/benchmarks_and_corpora/benchmarks/EN/webanno'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc529839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = \"emanjavacas/MacBERTh\"\n",
    "\n",
    "#model_architecture = \"bert-base-uncased\"\n",
    "#model_architecture = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_value = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c458f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae353531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d560b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    " \n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "count=0\n",
    "\n",
    "print(os.walk(folder))\n",
    "\n",
    "dataframes = []\n",
    "for subfolder in sorted(os.listdir(folder)):\n",
    "        subfolder_path = subfolder\n",
    "        tables = []\n",
    "        subfolder_path = os.path.join(folder, subfolder_path)\n",
    "        for file in os.scandir(subfolder_path):\n",
    "            if file.is_file():\n",
    "                annotation_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "                table = pd.read_table(annotation_path,comment='#', error_bad_lines=False, engine=\"python\", header=None, quoting = 3, quotechar=None)\n",
    "                table = table.rename(columns={0: \"token_id\", 1: \"char_range\", 2: \"token\", 3: \"ref_type\"})\n",
    "                \n",
    "                if not 'ref_type' in table:\n",
    "                    continue\n",
    "                #table = table[~(table['ref_type'] == '_') & ~table['ref_type'].isna()]\n",
    "                table['filename'] = annotation_path.split('/')[-2]\n",
    "                tables.append(table)\n",
    "                print(annotation_path)\n",
    "                \n",
    "        dataframes.append(pd.concat(tables, ignore_index=True, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca58d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_folder = '/media/username/DATA_LINUX1/Datasets/Odeuropa/benchmarks_and_corpora/benchmarks/EN/source/source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "text_content = {}\n",
    "for file in sorted(os.listdir(docs_folder)):\n",
    "    with open(os.path.join(docs_folder, file)) as f:\n",
    "        lines = f.read()\n",
    "\n",
    "        lines = tokenize.sent_tokenize(lines)\n",
    "        lines =[line.replace('\\n', ' ') for line in lines]\n",
    "        lines =[line.replace('\\t', ' ') for line in lines]\n",
    "        text_content[file] = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc897b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs['sentence_id'] = refs['token_id'].apply(lambda x : x.split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = refs.drop_duplicates(subset=['filename', 'sentence_id', 'token_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d6b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = refs[~refs['ref_type'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101257bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_type(ref_type):\n",
    "    if '[' in ref_type:\n",
    "        return str(ref_type).split('[')[0]\n",
    "    else:\n",
    "        return ref_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs['ref_type'] = refs['ref_type'].apply(lambda x : 'O' if x == '_' else '1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5132a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = refs.reset_index(drop='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe05319",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = refs.groupby(['filename', 'sentence_id'], as_index = False).agg({'token':  ' '.join})\n",
    "encodings = refs.groupby(['filename', 'sentence_id'], as_index = False).agg({'ref_type': ' '.join}).rename(columns = {'ref_type':'labels'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62efa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_beginning_and_intermediate_token(label):\n",
    "    tokens = label.split()\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] != 'O' :\n",
    "            tokens[i] = 'B-'+ tokens[i]\n",
    "            \n",
    "    for i in range(len(tokens)-1):\n",
    "        if tokens[i] != 'O' and tokens[i+1] != 'O':\n",
    "            if tokens[i+1].replace(\"B-\", '') in tokens[i]:\n",
    "                tokens[i+1] = tokens[i+1].replace(\"B-\", \"I-\")\n",
    "            \n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62357dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings['labels'] = encodings['labels'].apply(mark_beginning_and_intermediate_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470504e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(sentences, encodings, on=[\"filename\", \"sentence_id\"])\n",
    "df = df.rename(columns={'token': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_ref(labels):\n",
    "    labels = labels.split()\n",
    "    for label in labels:\n",
    "        if label != \"O\":\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contains_ref'] = df['labels'].apply(contains_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b8ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contains_ref'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].to_list()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed414f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck(sentence):\n",
    "    spell = SpellChecker()\n",
    "    sentence = sentence.split()\n",
    "    \n",
    "    corrected_sentence = [spell.correction(word) if spell.correction(word) is not None else word for word in sentence ]\n",
    "    return ' '.join(corrected_sentence) if corrected_sentence is not None else sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff70435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalize(comment, lowercase=True, remove_stopwords=True):\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = spacy_model(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not word.is_stop and not word.is_punct and word.pos_ in ['NOUN', 'ADJ', 'ADV', 'VERB']:\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f83b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy\n",
    "spacy_model = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee98b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removing the new line characters\n",
    "# with open('sound_sentences_17_25_words_300.csv') as f:\n",
    "#     lines = [line.rstrip() for line in f]\n",
    " \n",
    "# lines = [line for line in lines if line !='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sentences = pd.DataFrame({'text':lines})\n",
    "generated_sentences['spellchecked_text'] = generated_sentences['text'].progress_apply(spellcheck)\n",
    "generated_sentences['normalized_text'] = generated_sentences['spellchecked_text'].progress_apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c9b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sentences['contains_ref'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d136be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utterance_words = [str(x).split() for  x in train_data['normalized_text'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360aabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ut = []\n",
    "\n",
    "# for l in utterance_words:\n",
    "#     ut+=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6187904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct = 0\n",
    "# for l in ut:\n",
    "    \n",
    "#     if sensory_tag_token(l, 'Visual', dict_lexicon) != 0:\n",
    "#         ct+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct/len(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ut =list( dict.fromkeys(ut) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#df['spellchecked_text'] = df['text'].progress_apply(spellcheck)\n",
    "#df['normalized_text'] = df['spellchecked_text'].progress_apply(normalize)\n",
    "\n",
    "df = pd.read_csv('cleansed_odeuropa_sentences.csv')\n",
    "df = pd.concat([df, generated_sentences], axis=0).reset_index(drop=True)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=random_value)\n",
    "#train_data, train_mlp_data = train_test_split(train_data, test_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['contains_ref'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb255e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {False: 0, True:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_architecture, do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1acb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_data['text'].to_list()\n",
    "train_labels = [mapping[x] for x in train_data['contains_ref'].to_list()]\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in train_sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd72aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensory_tag_token(token, sensoriality, dict_lexicon):\n",
    "    if token in dict_lexicon:\n",
    "        return dict_lexicon[token][sensoriality]\n",
    "    else:\n",
    "        synonyms = wordnet.synsets(token)\n",
    "        candidates = list(set(chain.from_iterable([word.lemma_names() for word in synonyms])))\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            if candidate in dict_lexicon:\n",
    "                return dict_lexicon[candidate][sensoriality]\n",
    "        \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensory_tag_sentence(sentence, nlp):\n",
    "    s = 0\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    sensory_dict = {'AUD': 0, 'GUS': 0, 'HAP': 0, 'INT': 0, 'OLF': 0, 'VIS': 0, 'ARM': 0, \n",
    "                    'LEG': 0, 'TORSO':0, 'HEAD':0, 'MOUTH':0\n",
    "                   }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV', 'VERB']:\n",
    "            \n",
    "\n",
    "            \n",
    "            sensory_dict['AUD'] += sensory_tag_token(token.lemma_, 'Auditory', dict_lexicon)\n",
    "            sensory_dict['GUS'] += sensory_tag_token(token.lemma_, 'Gustatory', dict_lexicon)\n",
    "            sensory_dict['HAP'] += sensory_tag_token(token.lemma_, 'Haptic', dict_lexicon)\n",
    "            sensory_dict['INT'] += sensory_tag_token(token.lemma_, 'Interoceptive', dict_lexicon)\n",
    "            sensory_dict['OLF'] += sensory_tag_token(token.lemma_, 'Olfactory', dict_lexicon)\n",
    "            sensory_dict['VIS'] += sensory_tag_token(token.lemma_, 'Visual', dict_lexicon)\n",
    "            \n",
    "            sensory_dict['ARM'] += sensory_tag_token(token.lemma_, 'Hand_arm', dict_lexicon)\n",
    "            sensory_dict['LEG'] += sensory_tag_token(token.lemma_, 'Foot_leg', dict_lexicon)\n",
    "            sensory_dict['TORSO'] += sensory_tag_token(token.lemma_, 'Torso', dict_lexicon)\n",
    "            sensory_dict['HEAD'] += sensory_tag_token(token.lemma_, 'Head', dict_lexicon)\n",
    "            sensory_dict['MOUTH'] += sensory_tag_token(token.lemma_, 'Mouth', dict_lexicon)\n",
    "            \n",
    "            s+=1\n",
    "    \n",
    "#     for key in sensory_dict.keys():\n",
    "#         if s > 0:\n",
    "#             sensory_dict[key] /= s\n",
    "    return sensory_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08613bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data['text'].to_list()\n",
    "test_labels = [mapping[x] for x in test_data['contains_ref'].to_list()]\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in test_sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0252bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def normalize(comment, lowercase=True, remove_stopwords=True):\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = spacy_model(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            #if not word.is_stop and not word.is_punct and word.pos_ in ['NOUN', 'ADJ', 'ADV', 'VERB']:\n",
    "            lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_model = pd.read_csv('/media/username/DATA_LINUX1/Datasets/Lancaster/Lancaster_sensorimotor_norms_for_39707_words.csv')\n",
    "lexicon = lexicon_model[['Word', 'Auditory.mean', 'Gustatory.mean', 'Haptic.mean', 'Interoceptive.mean', 'Olfactory.mean', 'Visual.mean', 'Foot_leg.mean', 'Hand_arm.mean', 'Head.mean', 'Mouth.mean', 'Torso.mean']]\n",
    "lexicon = lexicon.rename(columns={\"Auditory.mean\": \"Auditory\", \"Gustatory.mean\": \"Gustatory\", \"Haptic.mean\": \"Haptic\", \"Interoceptive.mean\": \"Interoceptive\", \"Olfactory.mean\": \"Olfactory\", \"Visual.mean\": \"Visual\", 'Torso.mean': 'Torso', 'Mouth.mean': 'Mouth', 'Head.mean': 'Head', 'Foot_leg.mean': 'Foot_leg', 'Hand_arm.mean': 'Hand_arm'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ad3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon['Word'] = lexicon['Word'].apply(lambda x: x.lower())\n",
    "lexicon = lexicon.set_index(['Word'])\n",
    "dict_lexicon = lexicon.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edae6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['normalized_text'].to_list()\n",
    "split_captions= [str(caption).split(' ') for caption in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d5f424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "fasttext_path = './wiki.en.bin'\n",
    "fasttext_model = fasttext.load_model(fasttext_path)\n",
    "fasttext_model.get_nearest_neighbors('dog', k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bdd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a532066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "train_input_ids = []\n",
    "train_attention_masks = []\n",
    "train_sensorimotor_embeddings = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in train_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    train_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    train_attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    train_sensorimotor_embeddings.append(list(sensory_tag_sentence(sent, spacy_model).values()))\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
    "train_attention_masks = torch.cat(train_attention_masks, dim=0)\n",
    "train_sensorimotor_embeddings = torch.stack([torch.tensor(sent) for sent in train_sensorimotor_embeddings])\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', train_sentences[0])\n",
    "print('Token IDs:', train_input_ids[0])\n",
    "\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "test_sensorimotor_embeddings = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in test_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "    test_sensorimotor_embeddings.append(list(sensory_tag_sentence(sent, spacy_model).values()))\n",
    "\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "test_sensorimotor_embeddings = torch.stack([torch.tensor(sent) for sent in test_sensorimotor_embeddings])\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', test_sentences[0])\n",
    "print('Token IDs:', test_input_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf78d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([torch.tensor(sent) for sent in train_sensorimotor_embeddings]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affaa2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_sensorimotor_embeddings, train_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_sensorimotor_embeddings, test_labels)\n",
    "\n",
    "\n",
    "# # Create a 90-10 train-validation split.\n",
    "\n",
    "# # Calculate the number of samples to include in each set.\n",
    "# train_size = int(0.9 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "\n",
    "# # Divide the dataset by randomly selecting samples.\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# print('{:>5,} training samples'.format(train_size))\n",
    "# print('{:>5,} validation samples'.format(val_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f96e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9131eece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b011fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import ModelOutput\n",
    "from transformers import  BertConfig, BertPreTrainedModel, BertModel\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "class SequenceClassifierOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of sentence classification models.\n",
    "\n",
    "    Args:\n",
    "        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
    "            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
    "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
    "            sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss, MSELoss\n",
    "\n",
    "linear_size = 32\n",
    "\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size + 11, config.num_labels)\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(config.hidden_size + 11, linear_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(linear_size, linear_size),\n",
    "#             nn.ReLU(),            \n",
    "#             nn.Linear(linear_size, 2)\n",
    "#         )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "#     @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "#     @add_code_sample_docstrings(\n",
    "#         checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n",
    "#         output_type=SequenceClassifierOutput,\n",
    "#         config_class=_CONFIG_FOR_DOC,\n",
    "#         expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n",
    "#         expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n",
    "#     )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        sensorimotor_embeddings: Optional[torch.Tensor] = None\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(torch.cat((pooled_output, sensorimotor_embeddings), 1))\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"emanjavacas/MacBERTh\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#     num_labels = 11, # The number of output labels--2 for binary classification.\n",
    "#                     # You can increase this for multi-class tasks.   \n",
    "#     output_attentions = False, # Whether the model returns attentions weights.\n",
    "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "# )\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_architecture, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d0c09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca50275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1cf4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd73679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.optim import Adam\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 30\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Linear(config['hidden_size'], config['hidden_size'])\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pooled_outputs(bert, input_ids, attention_masks, token_type_ids=None,):\n",
    "    bert_pooler = BertPooler({'hidden_size':768})\n",
    "    bert_pooler.cuda()\n",
    "#     embedding_output = bert.bert.embeddings(input_ids, token_type_ids)\n",
    "#     print(embedding_output.shape)\n",
    "#     print(attention_masks.shape)\n",
    "#     encoded_layers = bert.bert.encoder(embedding_output,\n",
    "#                                   attention_masks.T)\n",
    "    \n",
    "#     sequence_output = encoded_layers[-1]\n",
    "    sequence_output = model(input_ids.to('cuda'), attention_masks.to('cuda'))[1][12]\n",
    "    pooled_output = bert_pooler(sequence_output)\n",
    "    encoded_layers = None\n",
    "    \n",
    "    return encoded_layers, pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b5408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c15dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e86d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = random_value\n",
    "\n",
    "# bert_pooler = BertPooler({'hidden_size':768})\n",
    "# bert_pooler.cuda()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "best_accuracy = 0\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    train_hidden_states = []\n",
    "    test_hidden_states = []\n",
    "    \n",
    "    t_train_hidden_states = []\n",
    "    t_test_hidden_states = []\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    \n",
    "    train_true = []\n",
    "    train_preds = []\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_sensorimotor_emb = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        \n",
    "        # #         sequence_output = model(b_input_ids.to('cuda'), b_input_mask.to('cuda'))[1][12]\n",
    "#         pooled_output = model.bert.pooler(model(b_input_ids.to('cuda'), b_input_mask.to('cuda'))[1][12]).to('cpu')\n",
    "#         train_hidden_states.append(pooled_output)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               sensorimotor_embeddings=b_sensorimotor_emb,\n",
    "                               labels=b_labels)\n",
    "\n",
    "# #         sequence_output = model(b_input_ids.to('cuda'), b_input_mask.to('cuda'))[1][12]\n",
    "#         pooled_output = model.bert.pooler(model(b_input_ids.to('cuda'), b_input_mask.to('cuda'))[1][12]).to('cpu')\n",
    "#         train_hidden_states.append(pooled_output)\n",
    "\n",
    "\n",
    "        loss = output['loss']\n",
    "        logits = output['logits']\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        train_true.append(label_ids)\n",
    "        train_preds.append(logits)\n",
    "            \n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    true = []\n",
    "    preds = []\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_sensorimotor_emb = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            output = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   sensorimotor_embeddings=b_sensorimotor_emb,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "#             sequence_output = model(b_input_ids.to('cuda'), b_input_mask.to('cuda'))[1][12].to('cpu')\n",
    "#             pooled_output = model.bert.pooler(model(b_input_ids.to('cuda'), b_input_mask.to('cuda'))[1][12]).to('cpu')\n",
    "#             test_hidden_states.append(pooled_output)\n",
    "\n",
    "            loss = output['loss']\n",
    "            logits = output['logits']\n",
    "                    # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            true.append(label_ids)\n",
    "            preds.append(logits)\n",
    "                        \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "\n",
    "        \n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    \n",
    "    if avg_val_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_val_accuracy\n",
    "        torch.save(model.state_dict(), 'best-model-parameters.pt') # official recommended\n",
    "        \n",
    "        best_preds = preds\n",
    "        best_true = true\n",
    "        \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46af5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_hidden_states = torch.cat(train_hidden_states, dim=0)\n",
    "# test_hidden_states = torch.cat(test_hidden_states, dim=0)\n",
    "\n",
    "# # t_train_hidden_states = torch.cat(t_train_hidden_states, dim=0)\n",
    "# # t_test_hidden_states = torch.cat(t_test_hidden_states, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318782de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b63150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = preds\n",
    "preds = [np.argmax(i, axis=1).flatten() for i in preds]\n",
    "preds = np.concatenate(preds)\n",
    "true = np.concatenate(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = preds\n",
    "best_preds = [np.argmax(i, axis=1).flatten() for i in best_preds]\n",
    "best_preds = np.concatenate(best_preds)\n",
    "best_true = np.concatenate(best_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff63809",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(true, preds)\n",
    "\n",
    "classes = list(mapping.keys())\n",
    "\n",
    "def_classes = []\n",
    "\n",
    "for indice in np.unique(true):\n",
    "    def_classes.append(classes[indice])\n",
    "\n",
    "# classes = ['Circumstances', 'Effect', 'Location', 'Perceiver', 'Quality', 'Smell\\\\_Source', 'Time']\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in def_classes],\n",
    "                     columns = [i for i in def_classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.savefig('{}_bs_{}_factorized_classes.png'.format(model_architecture.replace('/', '_'), batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e139462",
   "metadata": {},
   "outputs": [],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(best_true, best_preds, average=\"macro\", pos_label=1)\n",
    "recall = recall_score(best_true, best_preds, average=\"macro\", pos_label=1)\n",
    "\n",
    "print('Precision: ',precision)\n",
    "print('Recall: ',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_preds = [np.argmax(i, axis=1).flatten() for i in train_preds]\n",
    "# train_preds = np.concatenate(train_preds)\n",
    "# train_true = np.concatenate(train_true)\n",
    "\n",
    "# precision = precision_score(train_true, train_preds, average=\"macro\", pos_label=1)\n",
    "# recall = recall_score(train_true, train_preds, average=\"macro\", pos_label=1)\n",
    "\n",
    "# print('Precision: ',precision)\n",
    "# print('Recall: ',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3e640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel\n",
    "class ExtraBertMultiClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_path=model_architecture, labels_count=2, hidden_dim=768, mlp_dim=100, extras_dim=11, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = {\n",
    "            'bert_model_path': model_architecture,\n",
    "            'labels_count': 2,\n",
    "            'hidden_dim': 768,\n",
    "            'mlp_dim': 100,\n",
    "            'extras_dim': 11,\n",
    "            'dropout': 0.1,\n",
    "        }\n",
    "\n",
    "        self.bert = model\n",
    "        self.dropout = nn.Dropout(self.config['dropout'])\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.config['hidden_dim'] + self.config['extras_dim'], self.config['mlp_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.config['mlp_dim'], self.config['mlp_dim']),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(self.config['mlp_dim'], self.config['labels_count'])\n",
    "        )\n",
    "    \n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, tokens, masks, extras):\n",
    "#         _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        _, pooled_output = get_pooled_outputs(self.bert, tokens, masks)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        \n",
    "        concat_output = torch.cat((pooled_output, extras), dim=1)\n",
    "        mlp_output = self.mlp(concat_output)\n",
    "        # proba = self.sigmoid(mlp_output)\n",
    "        proba = self.softmax(mlp_output)\n",
    "\n",
    "        return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1555b04a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data['sensorimotor_representation'] = train_data.progress_apply(lambda x: sensory_tag_sentence(str(x.normalized_text), spacy_model), axis=1)\n",
    "test_data['sensorimotor_representation'] = test_data.progress_apply(lambda x: sensory_tag_sentence(str(x.normalized_text), spacy_model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11595cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_data.drop(['sensorimotor_representation'], axis=1), train_data['sensorimotor_representation'].apply(pd.Series)], axis=1)\n",
    "test_df = pd.concat([test_data.drop(['sensorimotor_representation'], axis=1), test_data['sensorimotor_representation'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['bert_pred'] = train_preds\n",
    "# test_df['bert_pred'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750737b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ 'GUS', 'HAP', 'AUD', 'OLF', 'VIS', 'INT', 'MOUTH', 'HEAD', 'TORSO', 'ARM', 'LEG']\n",
    "train_mlp_tensor = torch.tensor(train_df[features].values).float()\n",
    "test_mlp_tensor = torch.tensor(test_df[features].values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ddd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_mlp_tensor, train_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_mlp_tensor, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fcaa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_mlp_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_mlp_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = preds\n",
    "# preds = [np.argmax(i, axis=1).flatten() for i in preds]\n",
    "# preds = np.concatenate(preds)\n",
    "# true = np.concatenate(true)\n",
    "# precision = precision_score(true, preds, average=\"macro\", pos_label=1)\n",
    "# recall = recall_score(true, preds, average=\"macro\", pos_label=1)\n",
    "\n",
    "# print('Precision: ',precision)\n",
    "# print('Recall: ',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c0e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier, optimizer, train_dataloader):\n",
    "    epochs = 10\n",
    "    loss_func = nn.BCELoss()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        print(f'Epoch: {epoch_num + 1}/{epochs}')\n",
    "\n",
    "        # for step, batch in enumerate(tqdm_notebook(train_dataloader, desc=\"Iteration\")):\n",
    "        for step_num, batch_data in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "            # Full features\n",
    "            \n",
    "            token_ids, masks, extras, gold_labels = tuple(t.to(device) for t in batch_data)\n",
    "            probas = classifier(token_ids.to('cuda'), masks.to('cuda'), extras.to('cuda'))\n",
    "\n",
    "            batch_loss = loss_func(torch.argmax(probas, axis=1).type(torch.FloatTensor).to('cuda'), gold_labels.type(torch.FloatTensor).to('cuda'))\n",
    "            batch_loss.requires_grad = True\n",
    "            train_loss += batch_loss.item()\n",
    "            \n",
    "            classifier.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # clear_output(wait=True)\n",
    "\n",
    "        print(f'\\r{epoch_num} loss: {train_loss / (step_num + 1)}')\n",
    "\n",
    "        print(str(torch.cuda.memory_allocated(device) / 1000000) + 'M')\n",
    "\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(classifier, data_loader):\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "\n",
    "    output_ids = []\n",
    "    outputs = None\n",
    "    golden_labels_list = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step_num, batch_item in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
    "            \n",
    "            # Full features\n",
    "            token_ids, masks, extras, golden_labels = tuple(t.to(device) for t in batch_item)\n",
    "            probas = classifier(token_ids.to('cuda'), masks.to('cuda'), extras.to('cuda'))\n",
    "\n",
    "            numpy_logits = probas.cpu().detach().numpy()\n",
    "\n",
    "            if outputs is None:\n",
    "                outputs = numpy_logits\n",
    "            else:\n",
    "                outputs = np.vstack((outputs, numpy_logits))\n",
    "                \n",
    "                \n",
    "            golden_labels_np = golden_labels.cpu().detach().numpy()\n",
    "            \n",
    "            if golden_labels_list is None:\n",
    "                golden_labels_list = golden_labels_np\n",
    "            else:\n",
    "                golden_labels_list = np.concatenate((golden_labels_list, golden_labels_np), axis=None)\n",
    "\n",
    "            output_ids.append(None)\n",
    "\n",
    "    print(f'Evaluation completed for {len(outputs)} items')\n",
    "\n",
    "    return output_ids, outputs, golden_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64151fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = ExtraBertMultiClassifier()\n",
    "mlp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e91e4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27da09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb801bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = train(mlp, optimizer, train_mlp_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_ids, output, golden_labels_list = evaluation(mlp, validation_mlp_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172de2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_preds = torch.argmax(torch.tensor(output), axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d143ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_true = golden_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c28c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision = precision_score(mlp_true, mlp_preds, average=\"macro\", pos_label=1)\n",
    "# # recall = recall_score(mlp_true, mlp_preds, average=\"macro\", pos_label=1)\n",
    "\n",
    "# print('Precision: ',precision)\n",
    "# print('Recall: ',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d356318",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['class'] = train_df['contains_ref'].apply(lambda x: 1 if x else 0)\n",
    "test_df['class'] = test_df['contains_ref'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [ 'GUS', 'HAP', 'AUD', 'OLF', 'VIS', 'INT', 'MOUTH', 'HEAD', 'TORSO', 'ARM', 'LEG']\n",
    "\n",
    "# features = 'sensorimotor_representation'\n",
    "columns = 'fasttext_representation'\n",
    "#columns = features\n",
    "\n",
    "# train_X = train_data[columns]\n",
    "# train_X = np.asarray([x for x in train_X])\n",
    "# train_X = np.concatenate((train_X, train_data[features].to_numpy()), axis=1)\n",
    "\n",
    "train_X = train_df[features].to_numpy()\n",
    "train_y = train_df['class'].to_numpy().astype('int')\n",
    "\n",
    "# test_X = test_data[columns]\n",
    "# test_X = np.asarray([x for x in test_X])\n",
    "# test_X = np.concatenate((test_X, test_data[features].to_numpy()), axis=1)\n",
    "\n",
    "test_X = test_df[features].to_numpy()\n",
    "test_y = test_df['class'].to_numpy().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d027ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf =LogisticRegression()\n",
    "\n",
    "clf.fit(train_X, train_y)\n",
    "lr_preds = clf.predict(test_X).astype('int')\n",
    "precision = precision_score(test_y, lr_preds,  average=\"macro\")\n",
    "recall = recall_score(test_y, lr_preds,  average=\"macro\")\n",
    "f1 = f1_score(test_y, lr_preds, average=\"macro\")\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['bert_preds'] = best_preds\n",
    "test_data['lr_preds'] = lr_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655385d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['bert_and_lr_preds'] =  test_data['bert_preds'] + test_data['lr_preds'] >=1\n",
    "test_data['bert_and_lr_preds'] = test_data['bert_and_lr_preds'].apply(lambda x: 1 if x >0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dbed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(test_y, test_data['bert_preds'], average=\"macro\", pos_label=1)\n",
    "recall = recall_score(test_y, test_data['bert_preds'],  average=\"macro\", pos_label=1)\n",
    "f1 = f1_score(test_y, test_data['bert_preds'],  average=\"macro\", pos_label=1)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2cddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_precision = precision_score(test_y, test_data['bert_and_lr_preds'], average=\"macro\", pos_label=1)\n",
    "l_recall = recall_score(test_y, test_data['bert_and_lr_preds'],  average=\"macro\", pos_label=1)\n",
    "l_f1 = f1_score(test_y, test_data['bert_and_lr_preds'],  average=\"macro\", pos_label=1)\n",
    "\n",
    "print(l_precision)\n",
    "print(l_recall)\n",
    "print(l_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546845c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: ',l_precision - precision)\n",
    "print('Recall: ',l_recall - recall)\n",
    "print('F1:', l_f1 - f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a452f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env37last",
   "language": "python",
   "name": "env37last"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
