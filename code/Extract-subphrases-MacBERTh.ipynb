{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/media/username/DATA_LINUX1/Datasets/Odeuropa/benchmarks_and_corpora/benchmarks/EN/webanno'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dafb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = \"emanjavacas/MacBERTh\"\n",
    "\n",
    "#model_architecture = \"bert-base-uncased\"\n",
    "#model_architecture = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb636fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    " \n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "count=0\n",
    "\n",
    "print(os.walk(folder))\n",
    "\n",
    "dataframes = []\n",
    "for subfolder in sorted(os.listdir(folder)):\n",
    "        subfolder_path = subfolder\n",
    "        tables = []\n",
    "        subfolder_path = os.path.join(folder, subfolder_path)\n",
    "        for file in os.scandir(subfolder_path):\n",
    "            if file.is_file():\n",
    "                annotation_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "                table = pd.read_table(annotation_path,comment='#', error_bad_lines=False, engine=\"python\", header=None)\n",
    "                table = table.rename(columns={0: \"token_id\", 1: \"char_range\", 2: \"token\", 3: \"ref_type\"})\n",
    "                \n",
    "                if not 'ref_type' in table:\n",
    "                    continue\n",
    "                #table = table[~(table['ref_type'] == '_') & ~table['ref_type'].isna()]\n",
    "                table['filename'] = annotation_path.split('/')[-2]\n",
    "                tables.append(table)\n",
    "                print(annotation_path)\n",
    "                \n",
    "        dataframes.append(pd.concat(tables, ignore_index=True, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a42feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_folder = '/media/username/DATA_LINUX1/Datasets/Odeuropa/benchmarks_and_corpora/benchmarks/EN/source/source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "text_content = {}\n",
    "for file in sorted(os.listdir(docs_folder)):\n",
    "    with open(os.path.join(docs_folder, file)) as f:\n",
    "        lines = f.read()\n",
    "\n",
    "        lines = tokenize.sent_tokenize(lines)\n",
    "        lines =[line.replace('\\n', ' ') for line in lines]\n",
    "        lines =[line.replace('\\t', ' ') for line in lines]\n",
    "        text_content[file] = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b344e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4010ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77253ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs['ref_type'] = refs['ref_type'].apply(lambda x: str(x).split('[')[0] if x!= \"_\" else \"_\")\n",
    "refs['sentence_id'] = refs['token_id'].apply(lambda x : x.split('-')[0])\n",
    "refs = refs.drop_duplicates(subset=['filename', 'sentence_id', 'token_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs['ref_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a87c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs[refs['ref_type'] == 'Smell\\_Word']['token'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c871d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame_elements = refs.groupby(['filename', 'ref_type'], as_index = False).agg({'token': ' '.join})\n",
    "frame_elements = refs.groupby(['filename', 'sentence_id', 'ref_type'], as_index = False).agg({'token': ' '.join})\n",
    "sentences = refs.groupby(['filename', 'sentence_id' ], as_index = False).agg({'token': ' '.join}).rename(columns ={'token': 'sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde7f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ecb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_elements = frame_elements[(frame_elements['ref_type']!='nan') & (frame_elements['ref_type']!='_') & (frame_elements['ref_type']!='*') & (frame_elements['ref_type']!='None')]\n",
    "frame_elements = frame_elements[frame_elements['ref_type'].isin(['Smell\\\\_Source', 'Smell\\\\_Word', 'Quality', 'Odour\\\\_Carrier', 'Evoked\\_Odorant'])]\n",
    "# frame_elements = frame_elements[~frame_elements['ref_type'].isin(['Perceiver', 'Smell\\\\_Word'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = pd.merge(frame_elements, sentences, on=['filename', 'sentence_id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = frames['ref_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = frames[(frames['ref_type']!='nan') & (frames['ref_type']!='_') & (frames['ref_type']!='*') & (frames['ref_type']!='None')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f0acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames['ref_type'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames['ref_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec430d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = frames.groupby(['filename', 'sentence_id', 'sentence', 'ref_type'], as_index = False).agg({'token': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc13431",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = pd.merge(frames, frame_elements[['filename', 'sentence_id', 'ref_type']], on=['filename', 'sentence_id', 'ref_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03acf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72870fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = frames.drop_duplicates(subset=['filename', 'sentence_id'], keep='first').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f0fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_sentence(sequence, filename, dictionnary):\n",
    "    for sentence in dictionnary[filename]:\n",
    "        if sequence in sentence:\n",
    "            return sentence\n",
    "        \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_olfaction(sentence, fe_df):\n",
    "    return True if sentence in fe_df['sentence'].tolist() else False\n",
    "\n",
    "# sentences = []\n",
    "# for key in text_content.keys():\n",
    "#     sentences += [sentence for sentence in text_content[key]]\n",
    "    \n",
    "# sentences_df = pd.DataFrame(sentences, columns=['sentence'])\n",
    "# sentences_df['contains_ref'] = sentences_df['sentence'].apply(lambda x: contains_olfaction(x, frame_elements))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_elements = frames\n",
    "frame_elements['original_ref_type'] = frame_elements['ref_type']\n",
    "frame_elements['ref_type'] = 'Olfactive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83794cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aaada3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from operator import itemgetter, attrgetter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from string import punctuation\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from emoji import *\n",
    "# import emoji\n",
    "import functools\n",
    "import string\n",
    "import operator\n",
    "import random\n",
    "random.seed(50)\n",
    "##basic imports\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14724852",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_elements['len'] = frame_elements['sentence'].apply(lambda x: len(x.split()))\n",
    "frame_elements = frame_elements[frame_elements['len'] <= 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = frame_elements.sample(frac = 0.8, random_state=1).reset_index()\n",
    "test_data = frame_elements.drop(train_data.index).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de08dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[test_data['ref_type']=='Olfactive'].reset_index(drop=\"true\")\n",
    "train_data = train_data[train_data['ref_type']=='Olfactive'].reset_index(drop=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of the training data : \", train_data.shape)\n",
    "print(\"Number of data points in the train data : \" , train_data.shape[0])\n",
    "print(\"Number of feature in the train data : \" , train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2387beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of the test data : \", test_data.shape)\n",
    "print(\"Number of data points in the test data : \" , test_data.shape[0])\n",
    "print(\"Number of feature in the test data : \" , test_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec165ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer : AAIC\n",
    "def Distribution(data ,column_name, title):\n",
    "    # Initialize the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "    # we need the total number of data to find the percentage later.\n",
    "    total = len(data) * 1\n",
    "\n",
    "    # below code will simply plot bar plot where X axis is sentiment and y will simply count\n",
    "    ax = sns.countplot(x=column_name, data=data)\n",
    "\n",
    "\n",
    "    # each p of patches(which is from the countplot) has height(number of data point for a given class ),width.\n",
    "    # then pass p to annotate(it is used to show text) and computing % of data in each class , give x coord and y coord of rectangle\n",
    "    for p in ax.patches:\n",
    "            ax.annotate('{:.1f}%'.format(100*p.get_height()/total), (p.get_x(), p.get_height()))\n",
    "\n",
    "\n",
    "    # In th yaxis we are giving interval(11 interval) of datapoints \n",
    "    ax.yaxis.set_ticks(np.linspace(0, total, 11))\n",
    "\n",
    "\n",
    "    # adjust the ticklabel to the desired format, without changing the position of the ticks.\n",
    "    # map() need the function(what to do) and iterative\n",
    "    # below code : ax.yaxis.get_majorticklocs() - it will give 11 value from 0 to 27481 and we are dividing it with the total value which is 27481 after that we are getting 11 intervals with the percentile \n",
    "    ax.set_yticklabels(map('{:.1f}%'.format, 100*ax.yaxis.get_majorticklocs()/total))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe421c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Distribution(train_data ,'ref_type',\"Distribution of ref_type in training data\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Distribution(test_data ,'ref_type',\"Distribution of ref_type in test data\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2126fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate value\n",
    "# drop_duplicates () : this function return DataFrame with duplicate rows removed.\n",
    "#train_data = train_data.drop_duplicates(keep ='first' , inplace = False)\n",
    "print(train_data.shape)\n",
    "# check null value\n",
    "null_rows = train_data[train_data.isnull().any(1)]\n",
    "print(null_rows)\n",
    "train_data.dropna(inplace=True)\n",
    "null_rows = train_data[train_data.isnull().any(1)]\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "# There were some word which i think might useful in the analysis which are present in the stopword are removed (like dont, would not ,does not , not etc ..)\n",
    "stopwords= set([ 'im','the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about','between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', 'didn', 'doesn', 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', 'wouldn'])\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "#     text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = ' '.join(e.lower() for e in text.split() if e.lower() not in stopwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d53bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given each senetnce it will call the clean_text()\n",
    "train_data['clean_sentence'] = train_data['sentence'].apply(lambda x:clean_text(x))\n",
    "train_data['clean_token'] = train_data['token'].apply(lambda x:clean_text(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given each senetnce it will call the clean_text()\n",
    "test_data['clean_sentence'] = test_data['sentence'].apply(lambda x:clean_text(x))\n",
    "test_data['clean_token'] = test_data['token'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reset_index(drop='true')\n",
    "test_data = test_data.reset_index(drop='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9150dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['ref_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_len(text_data):\n",
    "    # create a list to store the lenght of the sentence\n",
    "    store = []\n",
    "    # iterate thrrough each loop\n",
    "    for i in text_data[:]:\n",
    "        #split each word and count them and append in store\n",
    "        store.append((len(str(i).split())))\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Number_of_word_in_text'] = word_len(train_data['sentence'])\n",
    "train_data[\"Number_of_word_in_select_text\"] = word_len(train_data['token'])\n",
    "train_data[\"difference_word_in_text_and_selected_text\"] = train_data['Number_of_word_in_text'] - train_data[\"Number_of_word_in_select_text\"]\n",
    "train_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2212949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check it out the distribution of Number_of_word_in_text and Number_of_word_in_select_text\n",
    "f, ax = plt.subplots(figsize=(12,6))\n",
    "ax = sns.kdeplot(train_data['Number_of_word_in_text'], shade= True)\n",
    "ax = sns.kdeplot(train_data[\"Number_of_word_in_select_text\"],shade= True )\n",
    "plt.title(\"distribution of Number_of_word_in_text and Number_of_word_in_select_text\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def common_word(data , num_word , color, column_name , sentiment):\n",
    "\n",
    "    # create a temp list and store word \n",
    "    lst= []\n",
    "    for i in data[data['ref_type']== sentiment][column_name][:]:\n",
    "        lst.append(str(i).split())\n",
    "    # create a object which store the Counter() : this function will count how many time a word occur in the corpus\n",
    "    cnt = Counter()\n",
    "    # iterate through each sentence\n",
    "    for sentence in lst[:]:\n",
    "        # iterate through each word\n",
    "        for word in sentence:\n",
    "            cnt[word] += 1\n",
    "    # create a data frame and pass the most common words \n",
    "    fr = pd.DataFrame(cnt.most_common(num_word))\n",
    "    # give name of the column\n",
    "    fr.columns= [\"frequent word\" , \"count\"]\n",
    "    # it will give the color \n",
    "    return fr.style.background_gradient(cmap= color) ,fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c81d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_elements['ref_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a2893",
   "metadata": {},
   "source": [
    "### Classifiy Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4709438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(train_data , train_data.token , test_size = .1,random_state=42) \n",
    "X_train , X_val , y_train , y_val = train_test_split(X_train , X_train.token , test_size = .1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfa0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"   remove @ , # , & , convert lower case , remove lower case\"\"\"\n",
    "    # as we will work on rnns it is better not to remove the stopwords\n",
    "    text = text.lower() # convert all text to lower case\n",
    "    text = re.sub(r\"(@[a-z]*)\", \"<mention>\", text) #remove any word start with @\n",
    "    text = re.sub(r\"(&[a-z;]*)\", \"<none>\", text) #remove any word start with &\n",
    "    text = re.sub(r\"(#[a-z;]*)\", \"<hash>\", text) #remove any word start with #\n",
    "    text = re.sub(r\"(http|https|ftp|ftps)\\:\\/\\/[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\\/\\S*)?\", \"<link>\", text)#remove LINKs\n",
    "    text = re.sub(r'https?://\\S+', '<link>', text) # remove https? links\n",
    "    text = re.sub(r\"(www.[a-z.\\/0-9]*)\", \"<link>\", text)#remove LINKs\n",
    "    return text\n",
    "\n",
    "def Transfrom_text(textT,textS,type):\n",
    "    \"\"\" this function will repalce to <tok> which are in both text and selected text\"\"\"\n",
    "    t = \" \".join(\"<tok>\" for i in range (len(textS.split()))) # take each sentence in selected text find the length and put <tok>\n",
    "    textT = textT.replace(textS,t) # replace those word wich are in both text and selected_text with <tok>\n",
    "    return textT # return those sentence\n",
    "\n",
    "def preprocess_text(df):\n",
    "    df['text_clean'] = df['clean_sentence'].apply(lambda x: clean_text(x)) # create clean text column\n",
    "    df['selected_text_clean'] = df['clean_sentence'].apply(lambda x: clean_text(x)) # create clean selected text column\n",
    "    df['target'] = pd.DataFrame([Transfrom_text(df['clean_sentence'][i],df['clean_token'][i],df['ref_type'][i]) for i in range(len(df))]) # apply the transform function to all 3 column\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['ref_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51457129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not considering the neutral tweet\n",
    "#train_df = X_train[X_train.sentiment != 'neutral']\n",
    "train_df = X_train\n",
    "train_df = train_df.reset_index(drop='true')\n",
    "\n",
    "#val_df = X_val[X_val.sentiment != 'neutral']\n",
    "val_df = X_val\n",
    "val_df = val_df.reset_index(drop='true')\n",
    "\n",
    "#X_test = X_test[X_test.sentiment != 'neutral']\n",
    "X_test = X_test\n",
    "X_test = X_test.reset_index(drop='true')\n",
    "\n",
    "\n",
    "X_train = preprocess_text(train_df)\n",
    "X_test = preprocess_text(val_df)\n",
    "X_val = preprocess_text(X_test)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train.shape, X_train.shape)\n",
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d41454",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['ref_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a24367",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ce672",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  below code snippet is for tokenization \n",
    "\n",
    "# now merge all words \n",
    "#t1 = pd.DataFrame({'A': test.text.values})\n",
    "t2 = pd.DataFrame({'A': X_train.text_clean})\n",
    "t3 = pd.DataFrame({'A': X_train.target})\n",
    "# print(t2)\n",
    "all_tokens = pd.concat([t2,t3],axis = 0)\n",
    "\n",
    "# create a tokenizer and fit all the words\n",
    "token = Tokenizer(num_words=54000,filters='')\n",
    "token.fit_on_texts(all_tokens.A) \n",
    "vocab_size = len(token.word_index)+1\n",
    "\n",
    "MAX_LEN = 35\n",
    "# give \"<tok>\" as 1\n",
    "for word,i in token.word_index.items():\n",
    "  if \"<tok>\" in word:\n",
    "    token.word_index[word] = token.word_index[\"<tok>\"]    \n",
    "# it will create a vector based on the sentence word on train text data\n",
    "train_seq_x = token.texts_to_sequences(X_train.text_clean)\n",
    "# pre padding\n",
    "train_pad_seq_x = pad_sequences(train_seq_x,maxlen=MAX_LEN)\n",
    "# create vector on train target data\n",
    "\n",
    "# it will create a vector based on the sentence word on train text data\n",
    "val_seq_x = token.texts_to_sequences(X_val.text_clean)\n",
    "# pre padding\n",
    "val_pad_seq_x = pad_sequences(val_seq_x,maxlen=MAX_LEN )\n",
    "# create vector on train target data\n",
    "\n",
    "# it will create a vector based on the sentence word on test text data\n",
    "test_seq_x = token.texts_to_sequences(X_test.text_clean)\n",
    "# pre padding\n",
    "test_pad_seq_x = pad_sequences(test_seq_x,maxlen=MAX_LEN)\n",
    "\n",
    "# create vector on train target data\n",
    "train_seq_y = token.texts_to_sequences(X_train.target)\n",
    "# # pre padding\n",
    "train_pad_seq_y = pad_sequences(train_seq_y,maxlen=MAX_LEN)\n",
    "\n",
    "# create vector on  target data\n",
    "val_seq_y = token.texts_to_sequences(X_val.target)\n",
    "# # pre padding\n",
    "val_pad_seq_y = pad_sequences(val_seq_y,maxlen=MAX_LEN)\n",
    "\n",
    "\n",
    "# in the target data put 1 and 0 (if tok is present put 1 and all other values to 0)\n",
    "train_pad_seq_y[train_pad_seq_y != token.word_index['<tok>']] = 0\n",
    "train_pad_seq_y[train_pad_seq_y == token.word_index['<tok>']] = 1 \n",
    "\n",
    "# in the target data put 1 and 0 (if tok is present put 1 and all other values to 0)\n",
    "val_pad_seq_y[val_pad_seq_y != token.word_index['<tok>']] = 0\n",
    "val_pad_seq_y[val_pad_seq_y == token.word_index['<tok>']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using predefined glove vector\n",
    "def embedding_matrix_glove():\n",
    "  # to track the progress of computing\n",
    "  tqdm.pandas()\n",
    "  # create a viable to store glove vector\n",
    "  f = open('glove.840B.300d.txt')\n",
    "  # create a dictionary to \n",
    "  embedding_values = {}\n",
    "  for line in tqdm(f):\n",
    "    # take each word and the vector\n",
    "    value = line.split(' ')\n",
    "    # take the word\n",
    "    word = value[0]\n",
    "    # put all the numeric values to array\n",
    "    coef = np.array(value[1:],dtype = 'float32')\n",
    "    # take the word and array and put together\n",
    "    embedding_values[word] = coef\n",
    "\n",
    "  # take all the arrays from emmbeding values and stacked it\n",
    "  all_embs = np.stack(embedding_values.values())\n",
    "  # perform meand and standard deviation\n",
    "  emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "  #  create an embedding matrix of size (32515 ,300)\n",
    "  embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, 300))\n",
    "\n",
    "  lst = []\n",
    "  # for each word and given count take the embedding values\n",
    "  for word,i in tqdm(token.word_index.items()):\n",
    "      # now take the word get a vector from embedding values(arrays)\n",
    "      values = embedding_values.get(re.sub(r\"[^A-Za-z]\", \"\", word))\n",
    "      if values is not None:\n",
    "        embedding_matrix[i] = values\n",
    "      else:\n",
    "        lst.append(word) \n",
    "\n",
    "  return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensory_tag_token(token, sensoriality, dict_lexicon):\n",
    "    if token in dict_lexicon:\n",
    "        return dict_lexicon[token][sensoriality]\n",
    "    else:\n",
    "#         candidates = fasttext_model.get_nearest_neighbors(token, k=3)\n",
    "        \n",
    "#         for candidate in candidates:\n",
    "#             lemma = candidate[1]\n",
    "#             lemma = spacy_model(lemma)\n",
    "#             lemma = lemma[0].lemma_\n",
    "#             if lemma in dict_lexicon:\n",
    "#                 return dict_lexicon[lemma][sensoriality]\n",
    "        \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c035e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "spacy_model = spacy.load(\"en_core_web_md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f01235",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_model = pd.read_csv('/media/username/DATA_LINUX1/Datasets/Lancaster/Lancaster_sensorimotor_norms_for_39707_words.csv')\n",
    "lexicon = lexicon_model[['Word', 'Auditory.mean', 'Gustatory.mean', 'Haptic.mean', 'Interoceptive.mean', 'Olfactory.mean', 'Visual.mean', 'Foot_leg.mean', 'Hand_arm.mean', 'Head.mean', 'Mouth.mean', 'Torso.mean']]\n",
    "lexicon = lexicon.rename(columns={\"Auditory.mean\": \"Auditory\", \"Gustatory.mean\": \"Gustatory\", \"Haptic.mean\": \"Haptic\", \"Interoceptive.mean\": \"Interoceptive\", \"Olfactory.mean\": \"Olfactory\", \"Visual.mean\": \"Visual\", 'Torso.mean': 'Torso', 'Mouth.mean': 'Mouth', 'Head.mean': 'Head', 'Foot_leg.mean': 'Foot_leg', 'Hand_arm.mean': 'Hand_arm'})\n",
    "\n",
    "lexicon['Word'] = lexicon['Word'].apply(lambda x: x.lower())\n",
    "lexicon = lexicon.set_index(['Word'])\n",
    "dict_lexicon = lexicon.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4236ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensorialities = ['Auditory', 'Gustatory', 'Haptic', 'Interoceptive','Olfactory', 'Visual', 'Hand_arm', 'Foot_leg', 'Torso', 'Head', 'Mouth']\n",
    "\n",
    "## using predefined glove vector\n",
    "def embedding_matrix_glove_sensorimotor():\n",
    "  # to track the progress of computing\n",
    "  tqdm.pandas()\n",
    "  # create a viable to store glove vector\n",
    "  f = open('glove.840B.300d.txt')\n",
    "  # create a dictionary to \n",
    "  embedding_values = {}\n",
    "  for line in tqdm(f):\n",
    "    # take each word and the vector\n",
    "    value = line.split(' ')\n",
    "    # take the word\n",
    "    word = value[0]\n",
    "    nlp = spacy_model(word)\n",
    "\n",
    "    coef = []\n",
    "    for sensoriality in sensorialities:\n",
    "        coef.append(sensory_tag_token(nlp[0].lemma_, sensoriality, dict_lexicon))\n",
    "        \n",
    "    # take the word and array and put together\n",
    "    embedding_values[word] = coef\n",
    "\n",
    "\n",
    "  # take all the arrays from emmbeding values and stacked it\n",
    "  all_embs = np.stack(embedding_values.values())\n",
    "  # perform meand and standard deviation\n",
    "  emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "  #  create an embedding matrix of size (32515 ,300)\n",
    "  embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, 11))\n",
    "\n",
    "  lst = []\n",
    "  # for each word and given count take the embedding values\n",
    "  for word,i in tqdm(token.word_index.items()):\n",
    "      # now take the word get a vector from embedding values(arrays)\n",
    "      values = embedding_values.get(re.sub(r\"[^A-Za-z]\", \"\", word))\n",
    "      if values is not None:\n",
    "        embedding_matrix[i] = values\n",
    "      else:\n",
    "        lst.append(word) \n",
    "\n",
    "  return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054fccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# gv  = embedding_matrix_glove_sensorimotor()\n",
    "# end = time.time()\n",
    "\n",
    "# print(end - start)\n",
    "gv = np.fromfile('embedding_matrix_glove.txt').reshape(3714, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is simply decoded the encoded words\n",
    "# take a variable which will store all the index word\n",
    "# reverse_input_char_index = token.index_word\n",
    "\n",
    "def decode_sequence(train_pad_seq_x,input_seq):\n",
    "  # create a storage space to store string\n",
    "  decoded_sentence = \"\"\n",
    "  for i in range(len(input_seq)):\n",
    "    # input seq basically it is the test data\n",
    "    if (input_seq[i] == 1 or input_seq[i] == 2):\n",
    "      if train_pad_seq_x[i] != 0:\n",
    "        # take those sentences which are not cotaining zeros and put in token.index_word which will iteratively decode the words\n",
    "        sampled_char = reverse_input_char_index[train_pad_seq_x[i]]\n",
    "        decoded_sentence += sampled_char + \" \"\n",
    "    \n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc88c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545dca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_fasttext(data_1, data_2, model):\n",
    "    sent1_emb = np.mean([model[x] for word in data_1 for x in word.split()], axis=0)\n",
    "    sent2_emb = np.mean([model[x] for word in data_2 for x in word.split()], axis=0)\n",
    "    return 1 - scipy.spatial.distance.cosine(sent1_emb, sent2_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f453a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(str1, str2):\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    results = bleu.compute(predictions=[str1], references=[str2])\n",
    "    return results['bleu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28896cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(str1, str2):\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    results = rouge.compute(predictions=[str1], references=[str2])\n",
    "    return results['rouge1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    # after fitting the model this function will be called , and it predict and rounded up\n",
    "    test_pred = model.predict(train_pad_seq_x).round().astype(int)\n",
    "    avg_jac = 0\n",
    "    avg_cos = 0\n",
    "    avg_prec = 0\n",
    "    avg_rec = 0\n",
    "    for i in range(len(train_pad_seq_x)):  \n",
    "        # now call the decode function , which will give some words and \n",
    "        st1 = decode_sequence(train_pad_seq_x[i],test_pred[i])\n",
    "        st2 = decode_sequence(train_pad_seq_x[i],train_pad_seq_y[i])\n",
    "        # call the jaccard function to meause the similarity\n",
    "        \n",
    "        pred_words = st1.split()\n",
    "        true_words = st2.split()\n",
    "        \n",
    "        avg_prec += len([x for x in pred_words if x in true_words])/len(pred_words) if len(pred_words) >0 else 0\n",
    "        avg_rec += len([x for x in true_words if x in pred_words])/len(true_words) if len(true_words) > 0 else 0\n",
    "        avg_jac += jaccard(st1,st2)\n",
    "        avg_cos += cosine_similarity_fasttext(st1,st2, fasttext_model)\n",
    "    print(\"Jac train sccore = \" , np.sum(avg_jac) / len(train_pad_seq_x))\n",
    "    print(\"Cos sim train sccore = \" , np.sum(avg_cos) / len(train_pad_seq_x))\n",
    "    print(\"Precision = \" , np.sum(avg_prec) / len(train_pad_seq_x))\n",
    "    print(\"Recall = \" , np.sum(avg_rec) / len(train_pad_seq_x))\n",
    "\n",
    "\n",
    "    test_pred = model.predict(val_pad_seq_x).round().astype(int)\n",
    "    avg_jac = 0\n",
    "    avg_cos = 0\n",
    "    avg_prec = 0\n",
    "    avg_rec = 0\n",
    "    \n",
    "    div_prec = 0\n",
    "    div_rec = 0\n",
    "    for i in range(len(val_pad_seq_x)):  \n",
    "        st1 = decode_sequence(val_pad_seq_x[i],test_pred[i])\n",
    "        st2 = decode_sequence(val_pad_seq_x[i],val_pad_seq_y[i])\n",
    "        \n",
    "        pred_words = st1.split()\n",
    "        true_words = st2.split()\n",
    "        print(pred_words)\n",
    "        print(true_words)\n",
    "        \n",
    "        \n",
    "        avg_jac += jaccard(st1,st2)\n",
    "        avg_cos += cosine_similarity(st1,st2, fasttext_model)\n",
    "        avg_prec += len([x for x in pred_words if x in true_words])\n",
    "        avg_rec += len([x for x in true_words if x in pred_words])\n",
    "        div_prec += len(pred_words)\n",
    "        div_rec += len(true_words)\n",
    "        avg_jac += jaccard(st1,st2)\n",
    "\n",
    "    print(\"Jac valid sccore = \",np.sum(avg_jac) / len(val_pad_seq_x))\n",
    "    print(\"Cos sim train score = \" , np.sum(avg_cos) / len(val_pad_seq_x))\n",
    "#     print(\"Precision = \" , np.sum(avg_prec) / len(val_pad_seq_x))\n",
    "#     print(\"Recall = \" , np.sum(avg_rec) / len(val_pad_seq_x))\n",
    "    print(\"Precision = \" , avg_prec / div_prec)\n",
    "    print(\"Recall = \" , avg_rec / div_rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import evaluate\n",
    "fasttext_path = './wiki.en.bin'\n",
    "fasttext_model = fasttext.load_model(fasttext_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_words(st, true):\n",
    "    st_split = st.split()\n",
    "    for word_2 in range(len(st_split)):\n",
    "        for word in true.lower().split():\n",
    "            if st_split[word_2] in word:\n",
    "                st_split[word_2] = word\n",
    "\n",
    "    st = ' '.join(st_split)\n",
    "    return st.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# https://stackoverflow.com/questions/51889378/how-to-use-keras-reducelronplateau\n",
    "def model_check_point_tensor_board():\n",
    "  earlystopper = EarlyStopping(patience=8, verbose=1 ,restore_best_weights = True)\n",
    "  checkpointer = ModelCheckpoint(filepath = 'model_1_1.h5',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True, save_weights_only = True)\n",
    "\n",
    "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=2, min_lr=0.00001, verbose=1 , cooldown=1 )\n",
    "  \n",
    "  log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "\n",
    "  return [earlystopper , checkpointer , reduce_lr , tensorboard_callback ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a280375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # refer : https://udibhaskar.github.io/practical-ml/debugging%20nn/neural%20network/overfit/underfit/2020/02/03/Effective_Training_and_Debugging_of_a_Neural_Networks.html#Check-whether-forward-propagation-is-correct-or-not\n",
    "# #optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction. SUM_OVER_BATCH_SIZE)\n",
    "\n",
    "# ##masked loss Eg for sequence output. \n",
    "# def maskedLoss(train_pad_seq_y, y_pred):\n",
    "#     #getting mask value\n",
    "#     mask = tf.math.logical_not(tf.math.equal(train_pad_seq_y, 0))\n",
    "    \n",
    "#     #calculating the loss\n",
    "#     loss_ = loss_function(train_pad_seq_y, y_pred)\n",
    "    \n",
    "#     #converting mask dtype to loss_ dtype\n",
    "#     mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "#     #applying the mask to loss\n",
    "#     loss_ = loss_*mask\n",
    "    \n",
    "#     #getting mean over all the values\n",
    "#     loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "#     #loss_= tf.reduce_mean(loss_)\n",
    "#     return loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75239bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##LSTM\n",
    "\n",
    "# ##fixing numpy RS\n",
    "# np.random.seed(42)\n",
    "\n",
    "# ##fixing tensorflow RS\n",
    "# tf.random.set_seed(32)\n",
    "# #\n",
    "# ##python RS\n",
    "# rn.seed(12)\n",
    "\n",
    "# HIDDEN_DIM= 256\n",
    "\n",
    "# inputs = Input(shape=(MAX_LEN, ), dtype='float32')\n",
    "\n",
    "# embedding_layer = Embedding(3714, 11,weights = [gv],trainable = False,name=\"Embedding_layer\")\n",
    "# encoder_LSTM_1 = Bidirectional(LSTM(HIDDEN_DIM,return_sequences=True,kernel_regularizer=regularizers.l2(0.01),kernel_initializer=tf.keras.initializers.he_normal(seed=26),recurrent_initializer=tf.keras.initializers.orthogonal(seed=54)))\n",
    "# # when the output is a sequence we should use time distributed layer\n",
    "# # to understand more please go through this libk : https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/\n",
    "# dense_layer_relu = TimeDistributed(Dense(64, activation='relu',kernel_initializer=tf.keras.initializers.he_normal(seed=45)))\n",
    "# #dense_layer_relu_1 = TimeDistributed(Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.he_normal(seed=45)))\n",
    "# dense_layer = TimeDistributed(Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45)))\n",
    "\n",
    "# encoder_embedding = embedding_layer(inputs)\n",
    "# Encoded_seq = encoder_LSTM_1(encoder_embedding)\n",
    "\n",
    "\n",
    "# outputs = dense_layer_relu(Encoded_seq)\n",
    "# outputs = dense_layer(outputs)\n",
    "\n",
    "# model = Model(inputs, outputs)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce4e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# adam = Adam(lr=0.00001)\n",
    "# model.compile(optimizer=adam,loss = maskedLoss,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81516b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_arch(model_):\n",
    "  #refere :  https://github.com/mmortazavi/EntityEmbedding-Working_Example/blob/master/EntityEmbedding.ipynb\n",
    "  from keras.utils import plot_model\n",
    "  import pydot_ng as pydot\n",
    "  plot_model(model_, show_shapes=True, show_layer_names=True, to_file='model_1.png')\n",
    "  from IPython.display import Image\n",
    "  return Image(retina=True, filename='model_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train_pad_seq_x,train_pad_seq_y,batch_size=64,epochs=1500,validation_data=(val_pad_seq_x,val_pad_seq_y),callbacks = model_check_point_tensor_board(),verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a964a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['accuracy'], 'r')\n",
    "# plt.plot(history.history['val_accuracy'], 'b')\n",
    "# plt.legend({'Train accuracy': 'r', 'Test accuracy':'b'})\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(history.history['loss'], 'r')\n",
    "# plt.plot(history.history['val_loss'], 'b')\n",
    "# plt.legend({'Train Loss': 'r', 'Test Loss':'b'})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['ref_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2899704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train[X_train['ref_type'] == 'Olfactive']\n",
    "# #X_test = X_test[X_test['sentiment'] == 'neutral']\n",
    "# X_val = X_val[X_val['ref_type']== 'Olfactive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5add0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reset_index()\n",
    "# #X_test = X_test.reset_index()\n",
    "# X_val = X_val.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1080ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# X_train = preprocess_text(X_train)\n",
    "# #X_test = preprocess_text(X_test)\n",
    "# X_val = preprocess_text(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693640ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##  below code snippet is for tokenization \n",
    "\n",
    "# # now merge all words \n",
    "# #t1 = pd.DataFrame({'A': test.text.values})\n",
    "# t2 = pd.DataFrame({'A': X_train.text_clean})\n",
    "# t3 = pd.DataFrame({'A': X_train.target})\n",
    "# # print(t2)\n",
    "# all_tokens = pd.concat([t2,t3],axis = 0)\n",
    "\n",
    "# # create a tokenizer and fit all the words\n",
    "# token = Tokenizer(num_words=54000,filters='')\n",
    "# token.fit_on_texts(all_tokens.A) \n",
    "# vocab_size = len(token.word_index)+1\n",
    "\n",
    "# MAX_LEN = 35\n",
    "# # give \"<tok>\" as 1\n",
    "# for word,i in token.word_index.items():\n",
    "#   if \"<tok>\" in word:\n",
    "#     token.word_index[word] = token.word_index[\"<tok>\"]    \n",
    "# # it will create a vector based on the sentence word on train text data\n",
    "# train_seq_x = token.texts_to_sequences(X_train.text_clean)\n",
    "# # pre padding\n",
    "# train_pad_seq_x = pad_sequences(train_seq_x,maxlen=MAX_LEN)\n",
    "# # create vector on train target data\n",
    "\n",
    "# # it will create a vector based on the sentence word on train text data\n",
    "# val_seq_x = token.texts_to_sequences(X_val.text_clean)\n",
    "# # pre padding\n",
    "# val_pad_seq_x = pad_sequences(val_seq_x,maxlen=MAX_LEN)\n",
    "# # create vector on train target data\n",
    "\n",
    "# # it will create a vector based on the sentence word on test text data\n",
    "# #test_seq_x = token.texts_to_sequences(X_test.text_clean)\n",
    "# # pre padding\n",
    "# #test_pad_seq_x = pad_sequences(test_seq_x,maxlen=MAX_LEN)\n",
    "\n",
    "# # create vector on train target data\n",
    "# train_seq_y = token.texts_to_sequences(X_train.target)\n",
    "# # # pre padding\n",
    "# train_pad_seq_y = pad_sequences(train_seq_y,maxlen=MAX_LEN)\n",
    "# print(train_pad_seq_y[:1])\n",
    "# # in the target data put 1 and 0 (if tok is present put 1 and all other values to 0)\n",
    "\n",
    "# train_pad_seq_y[train_pad_seq_y != token.word_index['<tok>']] = 0\n",
    "# train_pad_seq_y[train_pad_seq_y == token.word_index['<tok>']] = 1 \n",
    "\n",
    "# # create vector on train target data\n",
    "# val_seq_y = token.texts_to_sequences(X_val.target)\n",
    "# # # pre padding\n",
    "# val_pad_seq_y = pad_sequences(val_seq_y,maxlen=MAX_LEN)\n",
    "\n",
    "# # in the target data put 1 and 0 (if tok is present put 1 and all other values to 0)\n",
    "# val_pad_seq_y[val_pad_seq_y != token.word_index['<tok>']] = 0\n",
    "# val_pad_seq_y[val_pad_seq_y == token.word_index['<tok>']] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd4d37",
   "metadata": {},
   "source": [
    "## Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92dbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##  below code snippet is for tokenization \n",
    "\n",
    "# # now merge all words \n",
    "# #t1 = pd.DataFrame({'A': test.text.values})\n",
    "# t2 = pd.DataFrame({'A': X_train.text_clean})\n",
    "# t3 = pd.DataFrame({'A': X_train.target})\n",
    "# # print(t2)\n",
    "# all_tokens = pd.concat([t2,t3],axis = 0)\n",
    "\n",
    "# # create a tokenizer and fit all the words\n",
    "# token = Tokenizer(num_words=54000,filters='')\n",
    "# token.fit_on_texts(all_tokens.A) \n",
    "# vocab_size = len(token.word_index)+1\n",
    "\n",
    "# MAX_LEN = 35\n",
    "# # give \"<tok>\" as 1\n",
    "# for word,i in token.word_index.items():\n",
    "#   if \"<tok>\" in word:\n",
    "#     token.word_index[word] = token.word_index[\"<tok>\"]    \n",
    "# # it will create a vector based on the sentence word on train text data\n",
    "# train_seq_x = token.texts_to_sequences(X_train.text_clean)\n",
    "# # pre padding\n",
    "# train_pad_seq_x = pad_sequences(train_seq_x,maxlen=MAX_LEN)\n",
    "# # create vector on train target data\n",
    "\n",
    "# # it will create a vector based on the sentence word on train text data\n",
    "# val_seq_x = token.texts_to_sequences(X_val.text_clean)\n",
    "# # pre padding\n",
    "# val_pad_seq_x = pad_sequences(val_seq_x,maxlen=MAX_LEN )\n",
    "# # create vector on train target data\n",
    "\n",
    "# # it will create a vector based on the sentence word on test text data\n",
    "# test_seq_x = token.texts_to_sequences(X_test.text_clean)\n",
    "# # pre padding\n",
    "# test_pad_seq_x = pad_sequences(test_seq_x,maxlen=MAX_LEN)\n",
    "\n",
    "# # create vector on train target data\n",
    "# train_seq_y = token.texts_to_sequences(X_train.target)\n",
    "# # # pre padding\n",
    "# train_pad_seq_y = pad_sequences(train_seq_y,maxlen=MAX_LEN , value=-1)\n",
    "\n",
    "# # create vector on  target data\n",
    "# val_seq_y = token.texts_to_sequences(X_val.target)\n",
    "# # # pre padding\n",
    "# val_pad_seq_y = pad_sequences(val_seq_y,maxlen=MAX_LEN,value =-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab5d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = []\n",
    "# for i in train_pad_seq_y:\n",
    "#   c = []\n",
    "#   a.append(c)\n",
    "#   for j in i:\n",
    "#     if j != - 1 and j != token.word_index['<tok>']:\n",
    "#       j = 0\n",
    "#     c.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = []\n",
    "# for i in val_pad_seq_y:\n",
    "#   c = []\n",
    "#   b.append(c)\n",
    "#   for j in i:\n",
    "#     if j != -1 and j != token.word_index['<tok>']:\n",
    "#       j = 0\n",
    "#     c.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pad_seq_y = np.asarray(a)\n",
    "# val_pad_seq_y = np.asarray(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e890f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gv = np.fromfile('embedding_matrix_glove.txt').reshape(3714, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_pad_seq_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b02d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pad_seq_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fda8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pad_seq_y = tf.reshape(train_pad_seq_y , (train_pad_seq_y.shape[0], 35, 1))\n",
    "# val_pad_seq_y = tf.reshape(val_pad_seq_y , (val_pad_seq_y.shape[0], 35 , -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # refer : https://udibhaskar.github.io/practical-ml/debugging%20nn/neural%20network/overfit/underfit/2020/02/03/Effective_Training_and_Debugging_of_a_Neural_Networks.html#Check-whether-forward-propagation-is-correct-or-not\n",
    "# #optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction = tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "# ##masked loss Eg for sequence output. \n",
    "# def maskedLoss(train_pad_seq_y, y_pred):\n",
    "#     #tf.print(\"input y_i\" , train_pad_seq_y)\n",
    "#     #tf.print((\"output y_i\" , y_pred))\n",
    "#     #getting mask value\n",
    "#     mask = tf.math.logical_not(tf.math.equal(train_pad_seq_y, -1))\n",
    "#     mask = tf.squeeze(mask , 2)\n",
    "#     #tf.print(k)\n",
    "#     #tf.print(\"y_i_hat\" , y_pred)\n",
    "#     #tf.print(\"yi\" , train_pad_seq_y)\n",
    "#     #calculating the loss\n",
    "#     loss_ = loss_function(train_pad_seq_y, y_pred)\n",
    "#     #tf.print(\"loss\",loss_)\n",
    "#     #tf.print(\"mask\" , mask)\n",
    "\n",
    "#     #loss_ = tf.reshape(-1 , mask.shape)\n",
    "#     #converting mask dtype to loss_ dtype\n",
    "#     mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#     #applying the mask to loss\n",
    "#     #tf.print(\"mask vector\" , mask)\n",
    "#     #tf.print(\"loss value\" , loss_)\n",
    "\n",
    "#     loss = loss_ * mask\n",
    "#     #tf.print(\"loss*mask\" , loss)\n",
    "#     #getting mean over all the values\n",
    "#     l = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "#     #tf.print(\"masked loss\" , l)\n",
    "#     #loss_= tf.reduce_mean(loss_)\n",
    "#     return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d8038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class attention(Layer):\n",
    "    \n",
    "#     def __init__(self, return_sequences=True):\n",
    "#         self.return_sequences = return_sequences\n",
    "#         super(attention,self).__init__()\n",
    "        \n",
    "#     def build(self, input_shape):\n",
    "        \n",
    "#         self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "#                                initializer=\"normal\")\n",
    "#         self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "#                                initializer=\"zeros\")\n",
    "        \n",
    "#         super(attention,self).build(input_shape)\n",
    "        \n",
    "#     def call(self, x):\n",
    "        \n",
    "#         e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "#         a = K.softmax(e, axis=1)\n",
    "#         output = x*a\n",
    "        \n",
    "#         if self.return_sequences:\n",
    "#             return output\n",
    "        \n",
    "#         return K.sum(output, axis=1)\n",
    "\n",
    "#     def compute_output_shape(self,input_shape):\n",
    "#         return (input_shape[0],input_shape[-1])\n",
    "\n",
    "#     def get_config(self):\n",
    "#         return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dae6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##LSTM\n",
    "\n",
    "# ##fixing numpy RS\n",
    "# np.random.seed(42)\n",
    "\n",
    "# ##fixing tensorflow RS\n",
    "# tf.random.set_seed(32)\n",
    "\n",
    "# ##python RS\n",
    "# rn.seed(12)\n",
    "\n",
    "# HIDDEN_DIM= 256\n",
    "\n",
    "# inputs = Input(shape=(MAX_LEN, ), dtype='float32')\n",
    "\n",
    "# embedding_layer = Embedding(3714,11,weights = [gv],trainable = False,name=\"Embedding_layer\")\n",
    "# encoder_LSTM_1 = Bidirectional(LSTM(HIDDEN_DIM,return_sequences=True,kernel_regularizer=regularizers.l2(0.01),kernel_initializer=tf.keras.initializers.he_normal(seed=26),recurrent_initializer=tf.keras.initializers.orthogonal(seed=54)))\n",
    "# attention_ = attention(return_sequences= True)\n",
    "# layer_norm = tf.keras.layers.LayerNormalization(center=False, scale=False)\n",
    "# # when the output is a sequence we should use time distributed layer\n",
    "# # to understand more please go through this libk : https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/\n",
    "# dense_layer_relu = TimeDistributed(Dense(64, activation='relu',kernel_initializer=tf.keras.initializers.he_normal(seed=45)))\n",
    "# layer_norm1 = tf.keras.layers.LayerNormalization(center=False, scale=False)\n",
    "# drop = TimeDistributed(Dropout(0.4))\n",
    "# #dense_layer_relu_1 = TimeDistributed(Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.he_normal(seed=45)))\n",
    "# dense_layer = TimeDistributed(Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45)))\n",
    "# drop1 = TimeDistributed(Dropout(0.4))\n",
    "\n",
    "\n",
    "# encoder_embedding = embedding_layer(inputs)\n",
    "# Encoded_seq = encoder_LSTM_1(encoder_embedding)\n",
    "# att = attention_(Encoded_seq)\n",
    "# lay_norm = layer_norm(att)\n",
    "# outputs = drop1(lay_norm)\n",
    "# outputs = dense_layer_relu(outputs)\n",
    "# outputs = layer_norm1(outputs)\n",
    "# outputs = drop(outputs)\n",
    "# outputs = dense_layer(outputs)\n",
    "\n",
    "# model = Model(inputs, outputs)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam = Adam(lr=0.001)\n",
    "# model.compile(optimizer=adam,loss = maskedLoss,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409318d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train_pad_seq_x,train_pad_seq_y,batch_size=32,epochs=400,validation_data=(val_pad_seq_x,val_pad_seq_y),callbacks = model_check_point_tensor_board(),verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290255c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(val_pad_seq_x,val_pad_seq_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8171750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0368f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['accuracy'], 'r')\n",
    "# plt.plot(history.history['val_accuracy'], 'b')\n",
    "# plt.legend({'Train accuracy': 'r', 'Test accuracy':'b'})\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(history.history['loss'], 'r')\n",
    "# plt.plot(history.history['val_loss'], 'b')\n",
    "# plt.legend({'Train Loss': 'r', 'Test Loss':'b'})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cf47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train[X_train['ref_type'] == 'Olfactive']\n",
    "# #X_test = X_test[X_test['sentiment'] == 'neutral']\n",
    "# X_val = X_val[X_val['ref_type']== 'Olfactive']\n",
    "\n",
    "# X_train = X_train.reset_index()\n",
    "# #X_test = X_test.reset_index()\n",
    "# X_val = X_val.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##  below code snippet is for tokenization \n",
    "\n",
    "# # now merge all words \n",
    "# #t1 = pd.DataFrame({'A': test.text.values})\n",
    "# t2 = pd.DataFrame({'A': X_train.text_clean})\n",
    "# t3 = pd.DataFrame({'A': X_train.target})\n",
    "# # print(t2)\n",
    "# all_tokens = pd.concat([t2,t3],axis = 0)\n",
    "\n",
    "# # create a tokenizer and fit all the words\n",
    "# token = Tokenizer(num_words=54000,filters='')\n",
    "# token.fit_on_texts(all_tokens.A) \n",
    "# vocab_size = len(token.word_index)+1\n",
    "\n",
    "# MAX_LEN = 35\n",
    "# # give \"<tok>\" as 1\n",
    "# for word,i in token.word_index.items():\n",
    "#   if \"<tok>\" in word:\n",
    "#     token.word_index[word] = token.word_index[\"<tok>\"]    \n",
    "# # it will create a vector based on the sentence word on train text data\n",
    "# train_seq_x = token.texts_to_sequences(X_train.text_clean)\n",
    "# # pre padding\n",
    "# train_pad_seq_x = pad_sequences(train_seq_x,maxlen=MAX_LEN)\n",
    "# # create vector on train target data\n",
    "\n",
    "# # it will create a vector based on the sentence word on train text data\n",
    "# val_seq_x = token.texts_to_sequences(X_val.text_clean)\n",
    "# # pre padding\n",
    "# val_pad_seq_x = pad_sequences(val_seq_x,maxlen=MAX_LEN )\n",
    "# # create vector on train target data\n",
    "\n",
    "# # it will create a vector based on the sentence word on test text data\n",
    "# test_seq_x = token.texts_to_sequences(X_test.text_clean)\n",
    "# # pre padding\n",
    "# test_pad_seq_x = pad_sequences(test_seq_x,maxlen=MAX_LEN)\n",
    "\n",
    "# # create vector on train target data\n",
    "# train_seq_y = token.texts_to_sequences(X_train.target)\n",
    "# # # pre padding\n",
    "# train_pad_seq_y = pad_sequences(train_seq_y,maxlen=MAX_LEN , value=-1)\n",
    "\n",
    "# # create vector on  target data\n",
    "# val_seq_y = token.texts_to_sequences(X_val.target)\n",
    "# # # pre padding\n",
    "# val_pad_seq_y = pad_sequences(val_seq_y,maxlen=MAX_LEN,value =-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bc1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = []\n",
    "# for i in train_pad_seq_y:\n",
    "#   c = []\n",
    "#   a.append(c)\n",
    "#   for j in i:\n",
    "#     if j != - 1 and j != token.word_index['<tok>']:\n",
    "#       j = 0\n",
    "#     c.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d741e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = []\n",
    "# for i in val_pad_seq_y:\n",
    "#   c = []\n",
    "#   b.append(c)\n",
    "#   for j in i:\n",
    "#     if j != -1 and j != token.word_index['<tok>']:\n",
    "#       j = 0\n",
    "#     c.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pad_seq_y = np.asarray(a)\n",
    "# val_pad_seq_y = np.asarray(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8349e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #positive\n",
    "# validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca8983",
   "metadata": {},
   "source": [
    "## Classify with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d04984",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = \"emanjavacas/MacBERTh\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b373114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ee3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference of this transformer model : https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705\n",
    "# reference text preprocessing  : https://www.youtube.com/watch?v=XaQ0CBlQ4cY\n",
    "# defining the max length \n",
    "MAX_LEN = 303\n",
    "\n",
    "# creating a bytelevel tokenizer \n",
    "# this tokenizer has \"\" ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing \"\" these attributes\n",
    "# the byte level BPE tokenizer done something like subword tokenizer where it will break a signle word into two word , example: faster could be fast and ##er\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(vocab='vocab-roberta-base.json', \n",
    "    merges='merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True)\n",
    "\n",
    "sentiment_id = {'Olfactive': 1313, 'Non-Olfactive': 2430}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d82550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bunch of variable for preprocessing\n",
    "ct = train_data.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(ct):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    # take the text and selected text and bring them in a uniform manner (sentences)\n",
    "    text1 = \" \"+\" \".join(train_data.loc[k,'clean_sentence'].split())\n",
    "    text2 = \" \".join(train_data.loc[k,'clean_token'].split())\n",
    "    #if text2 has all the word which is in text1 it will return 0 otherwise -1\n",
    "    idx = text1.find(text2)\n",
    "    # create zero vector size of the length\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1)\n",
    "\n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    # offsets : create a bunch of sets ,put word  start and end number in a particular set \n",
    "    # here is sentence : \"understanding offset and idx\"\n",
    "    # the offsets are: [(0, 13), (13, 20), (20, 24), (24, 27), (27, 28)]\n",
    "    # idx :  basically the end number of the word\n",
    "    # the idx is : 28\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    # define a toks which will store  which basically store number from text2\n",
    "    toks = []\n",
    "    # go through the offsets\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        # sum them like if there are [(2,5),(5,7)] sum them like : 3 , 2 etc\n",
    "        sm = np.sum(chars[a:b])\n",
    "        # now if sum is greater than zero append to tok \n",
    "        # basically sm will be greater than zero where we have same text as selected text\n",
    "        if sm>0: toks.append(i) \n",
    "\n",
    "\n",
    "    # create a variable to store the sentiment of the particular sentence   \n",
    "    s_tok = sentiment_id[train_data.loc[k,'ref_type']]\n",
    "    # now in the input id which is a size of (27481, 96) repalce some value to : start with zero then put the encoded token\n",
    "    # then 2 , 2 then the sentiment number and then 2\n",
    "    #  example : lets take a sentence : \"understanding offset and idx\" :\n",
    "    # and the encoded numbers are \n",
    "    # now put :  [0 ,2969, 6147, 8, 13561, 1178,2,2 ,7974 ,2 ,.....upto the size (27481, 96) ]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    # create attention mask and put 1 to the len of encoded number plus 1\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "\n",
    "    # now if the toks is greater than zero go to the start token variabe and give 1 to the inital token and give 1 to the end token\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = test_data.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test_data.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    # tokenize the test data as same as train data, we have not given the start and end token (as it is only for training)\n",
    "    text1 = \" \"+\" \".join(test_data.loc[k,'clean_sentence'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test_data.loc[k,'ref_type']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84709be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##fixing numpy RS\n",
    "np.random.seed(42)\n",
    "\n",
    "##fixing tensorflow RS\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "##python RS\n",
    "rn.seed(12)\n",
    "def build_model():\n",
    "    # below three variable will going to feed into the TFRoberta model\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained('config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained('roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.05)(x[0]) \n",
    "#     x1 = x[0]\n",
    "    x1 = tf.keras.layers.Conv1D(filters=1 , kernel_size=1 , kernel_initializer=tf.keras.initializers.glorot_uniform(seed = 45))(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.05)(x[0]) \n",
    "#     x2 = x[0]\n",
    "    x2 = tf.keras.layers.Conv1D(filters=1 , kernel_size=1 , kernel_initializer=tf.keras.initializers.glorot_uniform(seed = 45))(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c390ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()\n",
    "#plot_arch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1028f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "cos_sim = []\n",
    "bleus = []\n",
    "rouges = []\n",
    "# create some out of fold and predicton time varaible\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "t_oof_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "t_oof_end = np.zeros((input_ids_t.shape[0],MAX_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model checkpoint and store the tensor\n",
    "import datetime\n",
    "# https://stackoverflow.com/questions/51889378/how-to-use-keras-reducelronplateau\n",
    "def model_check_point_tensor_board():\n",
    "  #val_loss is the sum of val_activation_loss and val_activation_1_loss. \n",
    "  #Those two losses are the losses associated with predicting selected_text start token and selected_text end token. \n",
    "  checkpointer =  tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "  \n",
    "  log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "\n",
    "  return [ checkpointer, tensorboard_callback ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef488151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c3fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d45e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n",
    "true_pred_mapping_train = {}\n",
    "# skf split will generate train and test data\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_data.ref_type.values)):\n",
    "    print(idxV)\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    # clear the session\n",
    "    K.clear_session()\n",
    "    # call the model\n",
    "    model = build_model()\n",
    "\n",
    "    # fit the model by passing the train and validation data   which have been defined as idxT and IdxV\n",
    "    # attension mask will tells roBERTa which tokens are meaningful so roBERTa can ignore the rest\n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=20, batch_size=8, verbose=DISPLAY, callbacks= model_check_point_tensor_board(),\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "\n",
    "    # The variables preds_start and preds_end are the predictions for the test set. \n",
    "    # Each of the 5 folds predicts the entire test set.\n",
    "    # Therefore we need to take average (fold1 + fold2 + fold3 + fold4 + fold5) / 5.0\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    all_cos = []\n",
    "    all_prec = []\n",
    "    all_rec = []\n",
    "\n",
    "    # go through the validation data and take the  max value from oof start and end (which has been prepicted earlier)\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "#             st = train_data.loc[k,'clean_sentence']\n",
    "            st = \"\"\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train_data.loc[k,'clean_sentence'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        \n",
    "        \n",
    "        st = complete_words(st, train_data.loc[k,'clean_sentence'])\n",
    "            \n",
    "        all.append(jaccard(st,train_data.loc[k,'clean_token']))\n",
    "        all_cos.append(cosine_similarity_fasttext(st,train_data.loc[k,'clean_token'], fasttext_model))\n",
    "        \n",
    "        pred_words = st.split()\n",
    "        true_words = train_data.loc[k,'clean_token'].split()\n",
    "        \n",
    "        if len(pred_words) >0:\n",
    "            all_prec.append(len([x for x in pred_words if x in true_words])/len(pred_words))\n",
    "        if len(true_words) > 0 : \n",
    "            all_rec.append(len([x for x in pred_words if x in true_words])/len(true_words))\n",
    "        true_pred_mapping_train[train_data.loc[k,'clean_token']] = st\n",
    "        #all_bleu.append(compute_bleu(st,train_data.loc[k,'token']))\n",
    "        #all_rouge.append(compute_bleu(st,train_data.loc[k,'token']))\n",
    "    jac.append(np.mean(all))\n",
    "    cos_sim.append(np.mean(all_cos))\n",
    "    #bleus.append(np.mean(all_bleus))\n",
    "    #rouges.append(np.mean(all_rouges))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print('>>>> FOLD %i Cos Similarity='%(fold+1),np.mean(np.mean(all_cos)))\n",
    "    print('>>>> FOLD %i Precision='%(fold+1),np.mean(all_prec))\n",
    "    print('>>>> FOLD %i Recall='%(fold+1),np.mean(all_rec))\n",
    "\n",
    "\n",
    "    #print('>>>> FOLD %i BLEU ='%(fold+1),np.mean(np.mean(all_bleu)))\n",
    "    #print('>>>> FOLD %i ROUGE ='%(fold+1),np.mean(np.mean(all_rouge)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred_mapping_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bunch of variable for preprocessing\n",
    "filtered_test_data = test_data[test_data['ref_type']=='Olfactive'].reset_index(drop=\"true\")\n",
    "ct = filtered_test_data.shape[0]\n",
    "input_ids_t = np.ones((ct, MAX_LEN), dtype='int32')\n",
    "attention_mask_t = np.zeros((ct, MAX_LEN), dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct, MAX_LEN), dtype='int32')\n",
    "\n",
    "\n",
    "for k in range(filtered_test_data.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    # tokenize the test data as same as train data, we have not given the start and end token (as it is only for training)\n",
    "    text1 = \" \"+\" \".join(filtered_test_data.loc[k,'clean_sentence'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[filtered_test_data.loc[k,'ref_type']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf9b12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10 ,shuffle=True,random_state=42)\n",
    "\n",
    "preds_folds = []\n",
    "true_folds = []\n",
    "mappings = []\n",
    "# skf split will generate train and test data\n",
    "for fold, (idxT, idxV) in enumerate(skf.split(input_ids_t, filtered_test_data.ref_type.values)):\n",
    "\n",
    "    idxV = np.concatenate((idxT, idxV), axis=None)\n",
    "    \n",
    "    true_pred_mapping = {}\n",
    "    print('#' * 25)\n",
    "    print('### FOLD %i' % (fold + 1))\n",
    "    print('#' * 25)\n",
    "    # clear the session\n",
    "#     K.clear_session()\n",
    "    # call the model\n",
    "#     model = build_model()\n",
    "\n",
    "    # fit the model by passing the train and validation data   which have been defined as idxT and IdxV\n",
    "    # attension mask will tells roBERTa which tokens are meaningful so roBERTa can ignore the rest\n",
    "    # model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]],\n",
    "    #    epochs=3, batch_size=32, verbose=DISPLAY, callbacks= model_check_point_tensor_board(),\n",
    "    #    validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],\n",
    "    #    [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "\n",
    "    # print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5' % (VER, fold))\n",
    "\n",
    "    print('Predicting OOF...')\n",
    "    t_oof_start[idxV,], t_oof_end[idxV,] = model.predict([input_ids_t[idxV,], attention_mask_t[idxV,], token_type_ids_t[idxV,]],\n",
    "                                                     verbose=DISPLAY)\n",
    "\n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t, attention_mask_t, token_type_ids_t], verbose=DISPLAY)\n",
    "\n",
    "    # The variables preds_start and preds_end are the predictions for the test set.\n",
    "    # Each of the 5 folds predicts the entire test set.\n",
    "    # Therefore we need to take average (fold1 + fold2 + fold3 + fold4 + fold5) / 5.0\n",
    "    # preds_start += preds[0] / skf.n_splits\n",
    "    # preds_end += preds[1] / skf.n_splits\n",
    "\n",
    "    \n",
    "    true_text = []\n",
    "    preds_text = []\n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    all_cos = []\n",
    "    all_prec = []\n",
    "    all_rec = []\n",
    "    # go through the validation data and take the  max value from oof start and end (which has been prepicted earlier)\n",
    "    for k in idxV:\n",
    "        a = np.argmax(t_oof_start[k,])\n",
    "        b = np.argmax(t_oof_end[k,])\n",
    "        if a > b:\n",
    "            st = filtered_test_data.loc[k, 'clean_sentence']\n",
    "        else:\n",
    "            text1 = \" \" + \" \".join(filtered_test_data.loc[k, 'clean_sentence'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a - 1:b])\n",
    "            \n",
    "        st = complete_words(st, test_data.loc[k,'clean_token'])\n",
    "\n",
    "        # Fix loop to restore partially extracted words\n",
    "        st_split = st.split()\n",
    "        \n",
    "        for word_2 in range(len(st_split)):\n",
    "                \n",
    "            for word in train_data.loc[k,'clean_token'].split():\n",
    "                \n",
    "                if st_split[word_2] in word:\n",
    "                    st_split[word_2] = word\n",
    "                    \n",
    "        st = ' '.join(st_split)\n",
    "        \n",
    "        #print(\"Pred: \", st)\n",
    "        #print(\"True: \", test_data.loc[k, 'token'])\n",
    "        all.append(jaccard(st, filtered_test_data.loc[k, 'clean_token']))\n",
    "        all_cos.append(cosine_similarity_fasttext(st, filtered_test_data.loc[k,'clean_token'], fasttext_model))\n",
    "        \n",
    "        pred_words = st.split()\n",
    "        true_words = test_data.loc[k,'clean_token'].split()\n",
    "        if len(pred_words) >0:\n",
    "            all_prec.append(len([x for x in pred_words if x in true_words])/len(pred_words))\n",
    "        if len(true_words) > 0 : \n",
    "            all_rec.append(len([x for x in pred_words if x in true_words])/len(true_words))\n",
    "\n",
    "        true_text.append(filtered_test_data.loc[k, 'clean_token'])\n",
    "        preds_text.append(st)\n",
    "        true_pred_mapping[filtered_test_data.loc[k, 'clean_token']] = st\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard =' % (fold + 1), np.mean(all))\n",
    "    print('>>>> FOLD %i Cos Similarity='%(fold+1),np.mean(np.mean(all_cos)))\n",
    "    print('>>>> FOLD %i Precision='%(fold+1),np.mean(all_prec))\n",
    "    print('>>>> FOLD %i Recall='%(fold+1),np.mean(all_rec))\n",
    "    print()\n",
    "    \n",
    "    mappings.append(true_pred_mapping)\n",
    "    preds_folds.append(preds_text)\n",
    "    true_folds.append(true_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d229104",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09505e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame_elements = refs.groupby(['filename', 'ref_type'], as_index = False).agg({'token': ' '.join})\n",
    "frames_original = refs.groupby(['filename', 'sentence_id', 'ref_type'], as_index = False).agg({'token': ' '.join})\n",
    "sentences = refs.groupby(['filename', 'sentence_id' ], as_index = False).agg({'token': ' '.join}).rename(columns ={'token': 'sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405adeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = pd.merge(frames_original, sentences, on=['filename', 'sentence_id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ae4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames['token'] = frames['token'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ed730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_prediction(mapping, token):\n",
    "#     for key in mapping.keys():\n",
    "#         if token in key:\n",
    "#             return mapping[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_mapping = {}\n",
    "# for m in mappings:\n",
    "#     full_mapping.update(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames['clean_sentence'] = frames['sentence'].apply(lambda x:clean_text(x))\n",
    "# #frames['pred'] = frames['clean_sentence'].apply(lambda x : full_mapping[x.lower()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05185157",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab83cca",
   "metadata": {},
   "source": [
    "### Check preds Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ad0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02638ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_pred_mapping_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43057154",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d50273",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pred_token']  = test_data['clean_token'].apply(lambda x: true_pred_mapping.get(x, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['pred_token']  = train_data['clean_token'].apply(lambda x: true_pred_mapping_train.get(x, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc48799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prec(pred, true):\n",
    "    pred_words = pred.split()\n",
    "    true_words = true.split()\n",
    "    \n",
    "    if len(pred_words) == 0:\n",
    "        return 0\n",
    "    return len([x for x in pred_words if x in true_words])/len(pred_words) \n",
    "        \n",
    "def get_rec(pred, true):\n",
    "    \n",
    "    pred_words = pred.split()\n",
    "    true_words = true.split()\n",
    "    if len(true_words) == 0:\n",
    "        return 0\n",
    "    return len([x for x in true_words if x in pred_words])/len(true_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29620c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['prec'] = test_data.apply(lambda x: get_prec(x.pred_token, x.clean_token), axis=1)\n",
    "test_data['rec'] = test_data.apply(lambda x: get_rec(x.pred_token, x.clean_token), axis=1)\n",
    "\n",
    "# test_data['div_prec'] = test_data.apply(lambda x: len(x.pred_token.split()), axis=1)\n",
    "# test_data['div_rec'] = test_data.apply(lambda x: len(x.clean_token.split()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04aa6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['rec'].quantile(q=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f346dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14da424",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train_data_keywords.csv')\n",
    "test_data.to_csv('test_data_keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100198dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data = pd.read_csv('train_data_keywords.csv')\n",
    "test_data = pd.read_csv('test_data_keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ecf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['rec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402de4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = \"emanjavacas/MacBERTh\"\n",
    "\n",
    "#model_architecture = \"bert-base-uncased\"\n",
    "#model_architecture = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_words(st1, st2):\n",
    "    split_1 = st1.split(' ')\n",
    "    split_2 = st2.split(' ')\n",
    "    \n",
    "    missing = []\n",
    "    for word in split_1:\n",
    "        if word not in split_2:\n",
    "            missing.append(word)\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa06b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['missing'] = test_data.apply(lambda x: get_missing_words(str(x.clean_token), str(x.pred_token)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['excedent'] = test_data.apply(lambda x: get_missing_words(str( x.pred_token), str(x.clean_token) ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa07bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['excedent'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd42a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def sensory_tag_token(token, sensoriality, dict_lexicon):\n",
    "    if token in dict_lexicon:\n",
    "        return dict_lexicon[token][sensoriality]\n",
    "    else:\n",
    "        synonyms = wordnet.synsets(token)\n",
    "        candidates = list(set(chain.from_iterable([word.lemma_names() for word in synonyms])))\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            if candidate in dict_lexicon:\n",
    "                return dict_lexicon[candidate][sensoriality]\n",
    "        \n",
    "        return 0\n",
    "\n",
    "def sensory_tag_sentence(sentence, nlp):\n",
    "    s = 0\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    sensory_dict = {'AUD': 0, 'GUS': 0, 'HAP': 0, 'INT': 0, 'OLF': 0, 'VIS': 0, 'ARM': 0, \n",
    "                    'LEG': 0, 'TORSO':0, 'HEAD':0, 'MOUTH':0\n",
    "                   }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV', 'VERB']:\n",
    "            \n",
    "\n",
    "            \n",
    "            sensory_dict['AUD'] += sensory_tag_token(token.lemma_, 'Auditory', dict_lexicon)\n",
    "            sensory_dict['GUS'] += sensory_tag_token(token.lemma_, 'Gustatory', dict_lexicon)\n",
    "            sensory_dict['HAP'] += sensory_tag_token(token.lemma_, 'Haptic', dict_lexicon)\n",
    "            sensory_dict['INT'] += sensory_tag_token(token.lemma_, 'Interoceptive', dict_lexicon)\n",
    "            sensory_dict['OLF'] += sensory_tag_token(token.lemma_, 'Olfactory', dict_lexicon)\n",
    "            sensory_dict['VIS'] += sensory_tag_token(token.lemma_, 'Visual', dict_lexicon)\n",
    "            \n",
    "            sensory_dict['ARM'] += sensory_tag_token(token.lemma_, 'Hand_arm', dict_lexicon)\n",
    "            sensory_dict['LEG'] += sensory_tag_token(token.lemma_, 'Foot_leg', dict_lexicon)\n",
    "            sensory_dict['TORSO'] += sensory_tag_token(token.lemma_, 'Torso', dict_lexicon)\n",
    "            sensory_dict['HEAD'] += sensory_tag_token(token.lemma_, 'Head', dict_lexicon)\n",
    "            sensory_dict['MOUTH'] += sensory_tag_token(token.lemma_, 'Mouth', dict_lexicon)\n",
    "            \n",
    "            s+=1\n",
    "    \n",
    "#     for key in sensory_dict.keys():\n",
    "#         if s > 0:\n",
    "#             sensory_dict[key] /= s\n",
    "    return sensory_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb91d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(A, B): \n",
    "    cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "import spacy \n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "spacy_model = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "sim_threshold = 0.95\n",
    "olf_threshold = 3\n",
    "\n",
    "def enrich_extracted_text(extracted_text, original_text):\n",
    "    \n",
    "    original_words = str(original_text).split()\n",
    "    extracted_words = str(extracted_text).split()\n",
    "    return_words = copy.deepcopy(extracted_words)\n",
    "\n",
    "    extracted_words_representations = []\n",
    "    \n",
    "    for word in extracted_words:\n",
    "        values = list(sensory_tag_token(word, 'AUD', dict_lexicon).values())\n",
    "        if values != [0]*11:\n",
    "            extracted_words_representations.append(values)        \n",
    "        \n",
    "    for original_word in original_words:\n",
    "\n",
    "        if original_word not in extracted_words:\n",
    "            sensorimotor_representation = sensory_tag_sentence(original_word, spacy_model)\n",
    "\n",
    "            for extracted_word in extracted_words_representations:\n",
    "                if cosine_sim(list(sensorimotor_representation.values()), extracted_word) >= sim_threshold and sensorimotor_representation['OLF'] >= olf_threshold :\n",
    "                    return_words.append(original_word)\n",
    "\n",
    "#             for extracted_word in extracted_words:\n",
    "#                  if cosine_similarity_fasttext(original_word, extracted_word, fasttext_model) >= sim_threshold and sensorimotor_representation['OLF'] >= olf_threshold :\n",
    "#                      return_words.append(original_word)\n",
    "                \n",
    "\n",
    "    return_words = ' '.join(return_words)\n",
    "    \n",
    "    return return_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e34c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['enriched_pred'] = train_data.apply(lambda x: enrich_extracted_text(x.pred_token, x.clean_sentence), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['enriched_pred'] = test_data.apply(lambda x: enrich_extracted_text(x.pred_token, x.clean_sentence), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b70b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['clean_token'] = test_data['clean_token'].apply(lambda x : str(x))\n",
    "test_data['clean_token'] = test_data['clean_token'].apply(lambda x : str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['enriched_prec'] = test_data.apply(lambda x: get_prec(x.enriched_pred, x.clean_token, ), axis=1)\n",
    "test_data['enriched_rec'] = test_data.apply(lambda x: get_rec(x.enriched_pred, x.clean_token, ), axis=1)\n",
    "\n",
    "test_data['word_count'] = test_data['clean_sentence'].apply(lambda x : len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a76f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[test_data['word_count']<1000]\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "\n",
    "x = test_data['prec'].to_list()\n",
    "y = test_data['rec'].to_list()\n",
    "z = test_data['word_count'].to_list()\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "points = ax.scatter(x, y, c=z, s=50, cmap=\"plasma\")\n",
    "f.colorbar(points)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[test_data['word_count']<1000]\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "\n",
    "x = test_data['enriched_prec'].to_list()\n",
    "y = test_data['enriched_rec'].to_list()\n",
    "z = test_data['word_count'].to_list()\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "points = ax.scatter(x, y, c=z, s=50, cmap=\"plasma\")\n",
    "f.colorbar(points)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_filtered = test_data[test_data['word_count']<=100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c8de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_filtered['enriched_prec'].mean())\n",
    "print(test_data_filtered[test_data_filtered['enriched_rec']<=1]['enriched_rec'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53797bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_filtered['prec'].mean())\n",
    "print(test_data_filtered['rec'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[test_data['enriched_pred']!= test_data['clean_sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('pred_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad84361",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data = test_data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data = test_preds_data.rename(columns={'token': 'true', 'pred_token': 'pred'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data['pred'] = test_preds_data.apply(lambda x: complete_words(x.pred, x.sentence), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_words('eglantine gillof', 'water eglantine gilloflowers smelling' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73965bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_preds_data.to_csv('test_preds_2.csv')\n",
    "# train_data.to_csv('train_preds_2.csv')\n",
    "\n",
    "# train_data = pd.read_csv('processed_train.csv')\n",
    "# test_preds_data = pd.read_csv('processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe18aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec51ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data['true'] = test_preds_data['true'].apply(lambda x : x.lower())\n",
    "test_preds_data['prec'] = test_preds_data.apply(lambda x: get_prec(str(x.pred), str(x.true)), axis = 1)\n",
    "test_preds_data['rec'] = test_preds_data.apply(lambda x: get_rec(str(x.pred), str(x.true)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2911e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data_filtered = test_preds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f02c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([x for x in test_preds_data_filtered['prec'].to_list()])/len([x for x in test_preds_data_filtered['prec'].to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b0d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([x for x in test_preds_data_filtered['rec'].to_list()])/len([x for x in test_preds_data_filtered['rec'].to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b93e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data = test_preds_data[test_preds_data['clean_token'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rec('cotton', 'white cotton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7556f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, HTML\n",
    "\n",
    "# display(HTML(export.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b19ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    " \n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "count=0\n",
    "\n",
    "print(os.walk(folder))\n",
    "\n",
    "dataframes = []\n",
    "for subfolder in sorted(os.listdir(folder)):\n",
    "        subfolder_path = subfolder\n",
    "        tables = []\n",
    "        subfolder_path = os.path.join(folder, subfolder_path)\n",
    "        for file in os.scandir(subfolder_path):\n",
    "            if file.is_file():\n",
    "                annotation_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "                table = pd.read_table(annotation_path,comment='#', error_bad_lines=False, engine=\"python\", header=None)\n",
    "                table = table.rename(columns={0: \"token_id\", 1: \"char_range\", 2: \"token\", 3: \"ref_type\"})\n",
    "                \n",
    "                if not 'ref_type' in table:\n",
    "                    continue\n",
    "                table = table[~(table['ref_type'] == '_') & ~table['ref_type'].isna()]\n",
    "                table['filename'] = annotation_path.split('/')[-2]\n",
    "                tables.append(table)\n",
    "                print(annotation_path)\n",
    "                \n",
    "        dataframes.append(pd.concat(tables, ignore_index=True, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'Circumstances': 0, 'Effect': 1, 'Evoked\\\\_Odorant':2 , 'Location': 3,\n",
    "       'Odour\\\\_Carrier': 4, 'Perceiver': 5, 'Quality': 6, 'Smell\\\\_Source': 7,\n",
    "       'Smell\\\\_Word': 8, 'Time': 9, '*': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62db5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_architecture, do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64643946",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[~train_data['token'].isna()]\n",
    "train_data = train_data[~train_data['original_ref_type'].isna()]\n",
    "train_data = train_data[~train_data['pred_token'].isna()]\n",
    "\n",
    "train_sentences = train_data['pred_token'].to_list()\n",
    "train_labels = [mapping[x] for x in train_data['original_ref_type'].to_list()]\n",
    "train_labels = [7 if x in [4, 2] else x for x in train_labels]\n",
    "# labels = [10 if x == 1 else x for x in labels]\n",
    "\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in train_sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = bert_tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d09902",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544202dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "train_input_ids = []\n",
    "train_attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in train_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = bert_tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    train_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    train_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
    "train_attention_masks = torch.cat(train_attention_masks, dim=0)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', train_sentences[0])\n",
    "print('Token IDs:', train_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef68a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_data = test_preds_data[~test_preds_data['original_ref_type'].isna()]\n",
    "# test_data = test_data[~test_data['pred'].isna()]\n",
    "# test_data = test_data[~test_data['true'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8063408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data = test_preds_data[~test_preds_data['pred'].isna()]\n",
    "test_sentences = test_preds_data['enriched_pred'].to_list()\n",
    "test_labels = [mapping[x] for x in test_preds_data['original_ref_type'].to_list()]\n",
    "test_labels = [7 if x in [4, 2] else x for x in test_labels]\n",
    "# labels = [10 if x == 1 else x for x in labels]\n",
    "\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in test_sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = bert_tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241284e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in test_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = bert_tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', test_sentences[0])\n",
    "print('Token IDs:', test_input_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91dec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbe91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1768f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_data['original_ref_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759bb928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"emanjavacas/MacBERTh\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#     num_labels = 11, # The number of output labels--2 for binary classification.\n",
    "#                     # You can increase this for multi-class tasks.   \n",
    "#     output_attentions = False, # Whether the model returns attentions weights.\n",
    "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "# )\n",
    "\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    model_architecture, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 11, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "bert_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(bert_model.parameters(),\n",
    "                  lr = 5e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 80\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a6a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda'\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    bert_model.train()\n",
    "\n",
    "    t_loader = train_dataloader\n",
    "    v_loader = validation_dataloader\n",
    "\n",
    "        \n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(t_loader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        bert_model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        output = bert_model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels)\n",
    "        loss = output['loss']\n",
    "        logits = output['logits']\n",
    "            \n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(t_loader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    bert_model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    true = []\n",
    "    preds = []\n",
    "          \n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in v_loader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            output = bert_model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = output['loss']\n",
    "            logits = output['logits']\n",
    "                    # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            true.append(label_ids)\n",
    "            preds.append(logits)\n",
    "                        \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13999c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [np.argmax(i, axis=1).flatten() for i in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.concatenate(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1af6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaacf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(true, preds)\n",
    "\n",
    "classes = list(mapping.keys())\n",
    "\n",
    "def_classes = []\n",
    "\n",
    "for indice in np.unique(true):\n",
    "    def_classes.append(classes[indice])\n",
    "\n",
    "# classes = ['Circumstances', 'Effect', 'Location', 'Perceiver', 'Quality', 'Smell\\\\_Source', 'Time']\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in def_classes],\n",
    "                     columns = [i for i in def_classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.savefig('{}_bs_{}_factorized_classes.png'.format(model_architecture.replace('/', '_'), batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548331fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "precision = precision_score(true, preds, average=\"macro\", pos_label=1)\n",
    "recall = recall_score(true, preds, average=\"macro\", pos_label=1)\n",
    "\n",
    "print('Precision: ',precision)\n",
    "print('Recall: ',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bf79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[test_data['clean_sentence'] == test_data['pred_token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b51809",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99ecf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f8890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env37last",
   "language": "python",
   "name": "env37last"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
